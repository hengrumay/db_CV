# from pyspark.ml.torch.distributor import TorchDistributor

# settings.update({"mlflow":True}) # if you do want to autolog.
# mlflow.autolog(disable = False)


# def train_fn(device_list = [0,1,2,3], yaml_path = None, active_run_id = None):

#     from ultralytics import YOLO
#     import torch
#     import mlflow
#     import torch.distributed as dist
#     from ultralytics import settings
#     from mlflow.types.schema import Schema, ColSpec
#     from mlflow.models.signature import ModelSignature

#     ############################

#     ##### Setting up MLflow ####
#     # We need to do this so that different processes that will be able to find mlflow
#     os.environ['DATABRICKS_HOST'] = db_host # pending replace with db vault secret
#     os.environ['DATABRICKS_TOKEN'] = db_token # pending replace with db vault secret 
#     os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = "true"
#     os.environ['MLFLOW_EXPERIMENT_NAME'] = experiment_name
#     os.environ['DATABRICKS_WORKSPACE_ID'] = db_wksp_id  # Set the workspace ID
#     # We set the experiment details here
#     experiment = mlflow.set_experiment(experiment_name)
    
#     #
#     with mlflow.start_run(run_id=active_run_id) as run:
#         model = YOLO(f"{project_location}/raw_model/yolov8n.pt")
#         model.train(
#             batch=8,
#             device=device_list,
#             data=yaml_path, # put your .yaml file path here.
#             epochs=3,
#             project=f'{tmp_project_location}',
#             exist_ok=True,
#             fliplr=1,
#             flipud=1,
#             perspective=0.001,
#             degrees=.45
#         )

#     # active_run_id = mlflow.last_active_run().info.run_id
#     # print("For YOLO autologging, active_run_id is: ", active_run_id)

#     # # after training is done.
#     # if not dist.is_initialized():
#     #   # import torch.distributed as dist
#     #   dist.init_process_group("nccl")

#     local_rank = int(os.environ["LOCAL_RANK"])
#     global_rank = int(os.environ["RANK"])
    
#     if global_rank == 0:

#         active_run_id = mlflow.last_active_run().info.run_id
#         print("For YOLO autologging, active_run_id is: ", active_run_id)

#         # Get the list of runs in the experiment
#         runs = mlflow.search_runs(experiment_names=[experiment_name], order_by=["start_time DESC"], max_results=1)

#         # Extract the latest run_id
#         if not runs.empty:
#             latest_run_id = runs.iloc[0].run_id
#             print(f"Latest run_id: {latest_run_id}")
#         else:
#             print("No runs found in the experiment.")


#         with mlflow.start_run(run_id=latest_run_id) as run:
#             mlflow.log_artifact(yaml_path, "input_data_yaml")
#             mlflow.log_params({"rank":global_rank})
#             mlflow.pytorch.log_model(model.trainer.best, "model", signature=signature)
#             # mlflow.pytorch.log_model(YOLO(str(model.trainer.best)), "model", signature=signature)

#             # yolo_wrapper = YOLOC(model.trainer.best)
#             # mlflow.pyfunc.log_model(
#             #     artifact_path="model",
#             #     artifacts={'model_path': str(model.trainer.save_dir), "best_point": str(model.trainer.best)},
#             #     python_model=yolo_wrapper,
#             #     signature=signature)

#     # clean up
#     if dist.is_initialized():
#         dist.destroy_process_group()

#     return "finished" # can return any picklable object


# if __name__ == "__main__":

#     #: use this experiment_name
#     mlflow.set_experiment(experiment_name)
#     experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id
#     # Reset MLFLOW_RUN_ID, so we dont bump into the wrong one.
#     if 'MLFLOW_RUN_ID' in os.environ:
#         del os.environ['MLFLOW_RUN_ID']

#     #: setting global env
#     # Set Databricks workspace URL and token
#     os.environ['DATABRICKS_HOST'] = db_host = 'https://adb-984752964297111.11.azuredatabricks.net'
#     os.environ['DATABRICKS_WORKSPACE_ID'] = db_wksp_id = '984752964297111'
#     os.environ['DATABRICKS_TOKEN'] = db_token = dbutils.secrets.get(scope="yyang_secret_scope", key="pat")

#     print(os.environ['DATABRICKS_HOST'])
#     print(os.environ['DATABRICKS_WORKSPACE_ID'])
#     print(os.environ['DATABRICKS_TOKEN']) # anything from vault would be redacted print.
#     #  
#     os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = "true"
#     print(f"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING set to {os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING']}")
#     os.environ['MLFLOW_EXPERIMENT_NAME'] = experiment_name
#     print(f"MLFLOW_EXPERIMENT_NAME set to {os.environ['MLFLOW_EXPERIMENT_NAME']}")
#     os.environ['DATABRICKS_WORKSPACE_ID'] = db_wksp_id  # Set the workspace ID
#     print(f"DATABRICKS_WORKSPACE_ID set to {os.environ['DATABRICKS_WORKSPACE_ID']}")


#     #: caculate # of GPUs
#     num_gpus = torch.cuda.device_count()
#     device_list = list(range(num_gpus))
#     print("num_gpus:", num_gpus)
#     print("device_list:", device_list)

#     yaml_path = yaml_path # reflect your dataset yolo format .yaml path.

#     with mlflow.start_run(experiment_id=experiment_id) as run:
#         active_run_id = mlflow.last_active_run().info.run_id
#         active_run_name = mlflow.last_active_run().info.run_name

#         print("For master triggering run, active_run_id is: ", active_run_id)
#         print("For master triggering run, active_run_name is: ", active_run_name)
#         print(f"For master triggering run, active_run_id is: '{active_run_id}' and active_run_name is: '{active_run_name}'.")
#         print(f"All worker runs will be logged into the same run id '{active_run_id}' and name '{active_run_name}'.")

#         distributor = TorchDistributor(num_processes=num_gpus, local_mode=True, use_gpu=True)      
#         distributor.run(train_fn, device_list = device_list, yaml_path = yaml_path, active_run_id = active_run_id)


# # Best Practice You Are! Previously we have 2 runs, one for master and the other for all workers.
# # 1. master run id for recording system metrics only
# # 2. Inner run id for auto-logging and manually logging other artifacts.

# # Now I have changed the code to merge outter run and inner run to be the same run id. less confusion.