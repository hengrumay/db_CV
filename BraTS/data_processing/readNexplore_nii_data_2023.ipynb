{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093e11ea-7db3-43da-b83e-d7ab85a5a23c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# refs \n",
    "# https://learnopencv.com/3d-u-net-brats/#aioseo-dataset-preprocessing (WRT BraTS dataset)\n",
    "\n",
    "# https://nipy.org/nibabel/nifti_images.html\n",
    "\n",
    "## NIFTI (brain imaging related but not everyone uses it; DICOM may be preferred)\n",
    "# https://github.com/DataCurationNetwork/data-primers/blob/main/Neuroimaging%20DICOM%20and%20NIfTI%20Data%20Curation%20Primer/neuroimaging-dicom-and-nifti-data-curation-primer.md\n",
    "# https://discovery.ucl.ac.uk/id/eprint/10146893/1/geometry_medim.pdf \n",
    "\n",
    "\n",
    "# test data\n",
    "# https://www.kaggle.com/datasets/aiocta/brats2023-part-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eed540dd-12d9-4f36-9da3-a367a4b341ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UC path \n",
    "# mmt_mlops_demos.cv.data\n",
    "# /Volumes/mmt_mlops_demos/cv/data/BraTS2021_00495/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d96c7f-77d7-4b73-8629-13a91d24c711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## to do -- convert some of the setup as a utils/config file  etc. \n",
    "\n",
    "!pip install nibabel -q\n",
    "!pip install scikit-learn -q\n",
    "!pip install tqdm -q\n",
    "!pip install split-folders -q\n",
    "!pip install torchinfo -q\n",
    "!pip install segmentation-models-pytorch-3d -q\n",
    "!pip install livelossplot -q\n",
    "!pip install torchmetrics -q\n",
    "!pip install tensorboard -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ad397d-4379-4789-a70f-5121cbedb132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09763bd8-4bc5-4c9e-a5c1-8ef0e88382c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Some of these might not be needed for dataprocessing"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import splitfolders\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import time\n",
    " \n",
    "from dataclasses import dataclass\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda import amp\n",
    " \n",
    "from torchmetrics import MeanMetric\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    " \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchinfo import summary\n",
    "import gc\n",
    " \n",
    "import segmentation_models_pytorch_3d as smp\n",
    " \n",
    "from livelossplot import PlotLosses\n",
    "from livelossplot.outputs import MatplotlibPlot, ExtremaPrinter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0680abd-3782-4d4e-9b29-c8ba0941cc4a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Download test data (BraTS2023) from Kaggle"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/aiocta/brats2023-part-1\n",
    "\n",
    "# Do ONCE\n",
    "# !pip install kaggle -q\n",
    "# !kaggle datasets download -d aiocta/brats2023-part-1 -p /Volumes/mmt_mlops_demos/cv/data/BraTS2023/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86dff492-bcf1-447b-aeb2-198e1da9c1f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Unzip BraTS2023 within UC Vols"
    }
   },
   "outputs": [],
   "source": [
    "# DO ONCE \n",
    "# !sudo apt install unzip\n",
    "# !unzip /Volumes/mmt_mlops_demos/cv/data/BraTS2023/brats2023-part-1.zip -d /Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023-Glioma/\n",
    "\n",
    "# !rm -rf <UD Vols path to>/brats2023-part-1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14cc6b40-b3ca-42d9-8ffe-6d655b50e194",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "maybe useful for later"
    }
   },
   "outputs": [],
   "source": [
    "# def seed_everything(SEED):\n",
    "#    np.random.seed(SEED)\n",
    "#    torch.manual_seed(SEED)\n",
    "#    torch.cuda.manual_seed_all(SEED)\n",
    "#    torch.backends.cudnn.deterministic = True\n",
    "#    torch.backends.cudnn.benchmark = False\n",
    " \n",
    " \n",
    "# def get_default_device():\n",
    "#    gpu_available = torch.cuda.is_available()\n",
    "#    return torch.device('cuda' if gpu_available else 'cpu'), gpu_available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16dbe55f-1ac2-4b7b-9034-72f4b83ff57e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Omitting because we are using YOLO instead"
    }
   },
   "outputs": [],
   "source": [
    "# @dataclass(frozen=True)\n",
    "# class TrainingConfig:\n",
    "#    BATCH_SIZE:      int = 5\n",
    "#    EPOCHS:          int = 100\n",
    "#    LEARNING_RATE: float = 1e-3\n",
    "#    CHECKPOINT_DIR:  str = os.path.join('model_checkpoint', '3D_UNet_Brats2023')\n",
    "#    NUM_WORKERS:     int = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00f1d1e-59bb-46c1-9e30-9fe3fec10ba3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "scaler = MinMaxScaler()"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    " \n",
    "DATASET_PATH = '/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023-Glioma/'\n",
    "print(\"Total Files: \", len(os.listdir(DATASET_PATH)))\n",
    "# Total Files:  625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df60930a-a60c-4a20-a380-5407a1785830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the NIfTI image\n",
    "sample_image_flair = nib.load(os.path.join(DATASET_PATH , \"BraTS-GLI-00000-000/BraTS-GLI-00000-000-t2f.nii\")).get_fdata()\n",
    "print(\"Original max value:\", sample_image_flair.max()) \n",
    "# Original max value: 2934.0\n",
    " \n",
    "# Reshape the 3D image to 2D for scaling\n",
    "sample_image_flair_flat = sample_image_flair.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b36c66c3-21cc-4c12-92f3-ca87b44dae15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_image_flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10cc3f8c-508c-4f3a-9d2b-29b9ae6b8baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_image_flair_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db57ce26-78c7-4632-b5f1-2cf4d0b02774",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "probably want to apply scaling  to the other image types?"
    }
   },
   "outputs": [],
   "source": [
    "# Apply scaling\n",
    "sample_image_flair_scaled = scaler.fit_transform(sample_image_flair_flat)\n",
    " \n",
    "# Reshape it back to the original 3D shape\n",
    "sample_image_flair_scaled = sample_image_flair_scaled.reshape(sample_image_flair.shape)\n",
    " \n",
    "print(\"Scaled max value:\", sample_image_flair_scaled.max())\n",
    "print(\"Shape of scaled Image: \", sample_image_flair_scaled.shape)\n",
    "\n",
    "# Scaled max value: 1.0\n",
    "# Shape of scaled Image:  (240, 240, 155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c776dab-c3b4-44d8-9c8b-8471b672b283",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "MASK info"
    }
   },
   "outputs": [],
   "source": [
    "sample_mask = nib.load(DATASET_PATH + \"BraTS-GLI-00000-000/BraTS-GLI-00000-000-seg.nii\").get_fdata()\n",
    "sample_mask = sample_mask.astype(np.uint8)  #\n",
    " \n",
    "print(\"Unique class in the mask\", np.unique(sample_mask)) \n",
    "print(\"Shape of sample_mask: \", sample_mask.shape)\n",
    "\n",
    "# Unique class in the mask [0 1 2 3]\n",
    "# Shape of sample_mask:  (240, 240, 155) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b6d8e8c-af28-4478-91da-6e58bbf57408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_image_t1 = nib.load(DATASET_PATH + \"BraTS-GLI-00000-000/BraTS-GLI-00000-000-t1n.nii\").get_fdata()\n",
    "# sample_image_t1 = sample_image_t1.astype(np.uint8)  # values between 0 and 255 | NOT NEEDED?\n",
    "\n",
    "\n",
    "sample_image_t1ce = nib.load(DATASET_PATH + \"BraTS-GLI-00000-000/BraTS-GLI-00000-000-t1c.nii\").get_fdata()\n",
    "# sample_image_t1c = sample_image_t1c.astype(np.uint8)  # values between 0 and 255 | NOT NEEDED?\n",
    "\n",
    "\n",
    "sample_image_t2 = nib.load(DATASET_PATH + \"BraTS-GLI-00000-000/BraTS-GLI-00000-000-t2w.nii\").get_fdata()\n",
    "# sample_image_t2 = sample_image_t2.astype(np.uint8)  # values between 0 and 255 |  NOT NEEDED?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9febf5b-3ccb-4beb-9700-ac94fd2aed27",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test randint"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the range\n",
    "low = 50\n",
    "high = 90 #141\n",
    "\n",
    "# Generate a random integer between low (inclusive) and high (exclusive)\n",
    "rand_int = np.random.randint(low, high)\n",
    "print(f\"Random integer between {low} and {high}: {rand_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "509d7b1d-16e9-4756-bb43-6ac6c62b7f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# n_slice = random.randint(0, sample_mask.shape[2])  # random slice between 0 - 154\n",
    "n_slice = np.random.randint(low, high) #77\n",
    "print(\"n_slice: \", n_slice)\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    " \n",
    "plt.subplot(231)\n",
    "plt.imshow(sample_image_flair_scaled[:,:,n_slice], cmap='gray')\n",
    "plt.title('Image flair')\n",
    " \n",
    "plt.subplot(232)\n",
    "plt.imshow(sample_image_t1[:,:,n_slice], cmap = \"gray\")\n",
    "plt.title(\"Image t1\")\n",
    " \n",
    "plt.subplot(233)\n",
    "plt.imshow(sample_image_t1ce[:,:,n_slice], cmap='gray')\n",
    "plt.title(\"Image t1ce\")\n",
    " \n",
    "plt.subplot(234)\n",
    "plt.imshow(sample_image_t2[:,:,n_slice], cmap = 'gray')\n",
    "plt.title(\"Image t2\")\n",
    " \n",
    "plt.subplot(235)\n",
    "plt.imshow(sample_mask[:,:,n_slice])\n",
    "plt.title(\"Seg Mask\")\n",
    " \n",
    "plt.subplot(236)\n",
    "plt.imshow(sample_mask[:,:,n_slice], cmap = 'gray')\n",
    "plt.title('Mask Gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c32ae0c-7596-4c6b-96c4-65c817a12d31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_x = np.stack(\n",
    "    [sample_image_flair_scaled, sample_image_t1ce, sample_image_t2], axis=3\n",
    ")  # along the last channel dimension.\n",
    "print(\"Shape of Combined x \", combined_x.shape)\n",
    "# Shape of Combined x  (240, 240, 155, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "627eae6e-0163-400e-9cc2-48f8dce2f938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_x = combined_x[56:184, 56:184, 13:141]\n",
    "print(\"Shape after cropping: \", combined_x.shape)\n",
    " \n",
    "sample_mask_c = sample_mask[56:184,56:184, 13:141]\n",
    "print(\"Mask shape after cropping: \", sample_mask_c.shape)\n",
    " \n",
    "#Shape after cropping:  (128, 128, 128, 3)\n",
    "#Mask shape after cropping:  (128, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "154c1141-51ef-4a74-af5b-73cc659ec318",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Curious what this 'looks' like hmm"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(combined_x[:,:,n_slice-12], # wrt to 13:141\n",
    "          #   cmap = 'gray'\n",
    "          )\n",
    "plt.title(\"combined_x\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(sample_mask_c[:,:,n_slice-12], # wrt to 13:141\n",
    "          #  cmap = 'gray'\n",
    "          )\n",
    "plt.title(\"sample_mask_c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd4b971-53eb-4c16-a568-6e842a49dad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_mask_cat  = F.one_hot(torch.tensor(sample_mask_c, dtype = torch.long), num_classes = 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd15806-4582-4b1e-a57a-9deae51359f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_mask_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a40769a-045a-400e-b9f0-f1dd4125a692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "t1ce_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*t1c.nii\"))\n",
    "t2_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*t2w.nii\"))\n",
    "flair_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*t2f.nii\"))\n",
    "mask_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*seg.nii\"))\n",
    " \n",
    "print(\"t1ce list: \", len(t1ce_list))\n",
    "print(\"t2 list: \", len(t2_list))\n",
    "print(\"flair list: \", len(flair_list))\n",
    "print(\"Mask list: \", len(mask_list))\n",
    "\n",
    "# t1ce list:  625\n",
    "# t2 list:  625\n",
    "# flair list:  625\n",
    "# Mask list:  625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10077a89-3b0d-4335-97ec-42035e7afe97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to continue with preprocessing for normal pytorch process and then try to convert to coco "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c534c56-7709-4a7f-88f0-0894525fd548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## DATASET Preprocessing test to pytorch dataloader \n",
    "# -- we will need to see how to reformat to coco/yolo friendly format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7c188e-e866-4334-be47-603587e121e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# '/'.join(f\"{DATASET_PATH}\".split(\"/\")[:-2])\n",
    "UCV_folderpath =  \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b12efe-bde2-4be0-b476-8e79bd51ea8a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "to distribute this via spark udf OR streaming?"
    }
   },
   "outputs": [],
   "source": [
    "## do once\n",
    "\n",
    "for idx in tqdm(\n",
    "    range(len(t2_list)), desc=\"Preparing to stack, crop and save\", unit=\"file\"\n",
    "):\n",
    "    temp_image_t1ce = nib.load(t1ce_list[idx]).get_fdata()\n",
    "    temp_image_t1ce = scaler.fit_transform(\n",
    "        temp_image_t1ce.reshape(-1, temp_image_t1ce.shape[-1])\n",
    "    ).reshape(temp_image_t1ce.shape)\n",
    " \n",
    "    temp_image_t2 = nib.load(t2_list[idx]).get_fdata()\n",
    "    temp_image_t2 = scaler.fit_transform(\n",
    "        temp_image_t2.reshape(-1, temp_image_t2.shape[-1])\n",
    "    ).reshape(temp_image_t2.shape)\n",
    " \n",
    "    temp_image_flair = nib.load(flair_list[idx]).get_fdata()\n",
    "    temp_image_flair = scaler.fit_transform(\n",
    "        temp_image_flair.reshape(-1, temp_image_flair.shape[-1])\n",
    "    ).reshape(temp_image_flair.shape)\n",
    " \n",
    "    temp_mask = nib.load(mask_list[idx]).get_fdata()\n",
    " \n",
    "    temp_combined_images = np.stack(\n",
    "        [temp_image_flair, temp_image_t1ce, temp_image_t2], axis=3\n",
    "    )\n",
    " \n",
    "    temp_combined_images = temp_combined_images[56:184, 56:184, 13:141]\n",
    "    temp_mask = temp_mask[56:184, 56:184, 13:141]\n",
    " \n",
    "    val, counts = np.unique(temp_mask, return_counts=True)\n",
    " \n",
    "    # If a volume has less than 1% of mask, we simply ignore to reduce computation\n",
    "    if (1 - (counts[0] / counts.sum())) > 0.01:\n",
    "        #         print(\"Saving Processed Images and Masks\")\n",
    "\n",
    "        ## applying 1-hot here \n",
    "        temp_mask = F.one_hot(torch.tensor(temp_mask, dtype=torch.long), num_classes=4)\n",
    "        os.makedirs(f\"{UCV_folderpath}BraTS2023_Preprocessed/input_data_3channels/images\", exist_ok=True)\n",
    "        os.makedirs(f\"{UCV_folderpath}BraTS2023_Preprocessed/input_data_3channels/masks\", \n",
    "                    exist_ok=True)\n",
    " \n",
    "        np.save(\n",
    "            f\"{UCV_folderpath}BraTS2023_Preprocessed/input_data_3channels/images/image_\"\n",
    "            + str(idx)\n",
    "            + \".npy\",\n",
    "            temp_combined_images,\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{UCV_folderpath}BraTS2023_Preprocessed/input_data_3channels/masks/mask_\"\n",
    "            + str(idx)\n",
    "            + \".npy\",\n",
    "            temp_mask,\n",
    "        )\n",
    " \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd131a58-4ff3-4fb5-b835-c2e46db19e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images_folder = f\"{UCV_folderpath}BraTS2023_Preprocessed/input_data_3channels/images\"\n",
    "print(len(os.listdir(images_folder)))\n",
    " \n",
    "masks_folder = f\"{UCV_folderpath}BraTS2023_Preprocessed/input_data_3channels/masks\"\n",
    "print(len(os.listdir(masks_folder)))\n",
    "\n",
    "# Images: 575\n",
    "# Masks: 575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ee35ca-ce8c-4603-a0a1-9a5d6ba77af3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "split into train / val"
    }
   },
   "outputs": [],
   "source": [
    "## do once\n",
    "\n",
    "input_folder = f\"{UCV_folderpath}BraTS2023_Preprocessed/input_data_3channels/\"\n",
    " \n",
    "output_folder = f\"{UCV_folderpath}BraTS2023_Preprocessed/input_data_128/\"\n",
    " \n",
    "splitfolders.ratio(\n",
    "    input_folder, output_folder, seed=42, ratio=(0.75, 0.25), group_prefix=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c529ea6-b35d-4c1e-8d25-f0630fe04926",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "NOT deleting files"
    }
   },
   "outputs": [],
   "source": [
    "# if os.path.exists(input_folder):\n",
    "#     shutil.rmtree(input_folder)\n",
    "#     print(f\"{input_folder} is removed\")\n",
    "# else:\n",
    "#     print(f\"{input_folder} doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3ba008d-830c-4921-a54d-f356a46de86a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "define a custom data loader?"
    }
   },
   "outputs": [],
   "source": [
    "class BraTSDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, normalization=True):\n",
    "        super().__init__()\n",
    " \n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.img_list = sorted(\n",
    "            os.listdir(img_dir)\n",
    "        )  # Ensure sorting to match images and masks\n",
    "        self.mask_list = sorted(os.listdir(mask_dir))\n",
    "        self.normalization = normalization\n",
    " \n",
    "        # If normalization is True, set up a normalization transform\n",
    "        if self.normalization:\n",
    "            self.normalizer = transforms.Normalize(\n",
    "                mean=[0.5], std=[0.5]\n",
    "            )  # Adjust mean and std based on your data\n",
    " \n",
    "    def load_file(self, filepath):\n",
    "        return np.load(filepath)\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "       image_path = os.path.join(self.img_dir, self.img_list[idx])\n",
    "       mask_path = os.path.join(self.mask_dir, self.mask_list[idx])\n",
    "       # Load the image and mask\n",
    "       image = self.load_file(image_path)\n",
    "       mask = self.load_file(mask_path)\n",
    " \n",
    "       # Convert to torch tensors and permute axes to C, D, H, W format (needed for 3D models)\n",
    "       image = torch.from_numpy(image).permute(3, 2, 0, 1)  # Shape: C, D, H, W\n",
    "       mask = torch.from_numpy(mask).permute(3, 2, 0, 1)  # Shape: C, D, H, W\n",
    "       \n",
    "       # Normalize the image if normalization is enabled\n",
    "       if self.normalization:\n",
    "           image = self.normalizer(image)\n",
    "       \n",
    "       return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "709e2f17-ba9a-438b-a421-010d21fda061",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LOAD data into dataloader"
    }
   },
   "outputs": [],
   "source": [
    "train_img_dir = f\"{UCV_folderpath}BraTS2023_Preprocessed/input_data_128/train/images\"\n",
    "train_mask_dir = f\"{UCV_folderpath}BraTS2023_Preprocessed/input_data_128/train/masks\"\n",
    " \n",
    "val_img_dir = f\"{UCV_folderpath}BraTS2023_Preprocessed/input_data_128/val/images\"\n",
    "val_mask_dir = f\"{UCV_folderpath}BraTS2023_Preprocessed/input_data_128/val/masks\"\n",
    " \n",
    "val_img_list = os.listdir(val_img_dir)\n",
    "val_mask_list = os.listdir(val_mask_dir)\n",
    " \n",
    "# Initialize datasets with normalization only\n",
    "train_dataset = BraTSDataset(train_img_dir, train_mask_dir, normalization=True)\n",
    "val_dataset = BraTSDataset(val_img_dir, val_mask_dir, normalization=True)\n",
    " \n",
    "# Print dataset statistics\n",
    "print(\"Total Training Samples: \", len(train_dataset))\n",
    "print(\"Total Val Samples: \", len(val_dataset))\n",
    "\n",
    "#Total Training Samples:  431\n",
    "#Total Val Samples:  144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b678a32-e98e-4db0-8823-6ece6375be25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check sizes"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False, num_workers=4)\n",
    "\n",
    "# Sanity Check\n",
    "images, masks = next(iter(train_loader))\n",
    "print(f\"Train Image batch shape: {images.shape}\")\n",
    "print(f\"Train Mask batch shape: {masks.shape}\")\n",
    "\n",
    "# Train Image batch shape: torch.Size([5, 3, 128, 128, 128])\n",
    "# Train Mask batch shape: torch.Size([5, 4, 128, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc36be49-740c-40c9-8fbf-2026d8831207",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Viz"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_slices(images, masks, num_slices=20):\n",
    "    batch_size = images.shape[0]\n",
    " \n",
    "    masks = torch.argmax(masks, dim=1)  # along the channel/class dim\n",
    " \n",
    "    for i in range(min(num_slices, batch_size)):\n",
    "        fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    " \n",
    "        middle_slice = images.shape[2] // 2\n",
    "        ax[0].imshow(images[i, 0, middle_slice, :, :], cmap=\"gray\")\n",
    "        ax[1].imshow(images[i, 1, middle_slice, :, :], cmap=\"gray\")\n",
    "        ax[2].imshow(images[i, 2, middle_slice, :, :], cmap=\"gray\")\n",
    "        ax[3].imshow(masks[i, middle_slice, :, :], cmap=\"viridis\")\n",
    "        ax[4].imshow(masks[i, middle_slice, :, :], cmap=\"gray\")\n",
    " \n",
    "        ax[0].set_title(\"T1ce\")\n",
    "        ax[1].set_title(\"FLAIR\")\n",
    "        ax[2].set_title(\"T2\")\n",
    "        ax[3].set_title(\"Seg Mask\")\n",
    "        ax[4].set_title(\"Mask - Gray\")\n",
    " \n",
    "        plt.show()\n",
    " \n",
    " \n",
    "visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e7de203-8c09-456e-934d-f616a8c09d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images, masks = next(iter(train_loader))\n",
    "visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57ad15a1-857e-437c-8c70-6f13f90de176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "readNexplore_nii_data_2023",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
