{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093e11ea-7db3-43da-b83e-d7ab85a5a23c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Refs/Notes"
    }
   },
   "outputs": [],
   "source": [
    "# refs \n",
    "# https://learnopencv.com/3d-u-net-brats/#aioseo-dataset-preprocessing (WRT BraTS dataset)\n",
    "\n",
    "# https://nipy.org/nibabel/nifti_images.html\n",
    "\n",
    "## NIFTI (brain imaging related but not everyone uses it; DICOM may be preferred)\n",
    "# https://github.com/DataCurationNetwork/data-primers/blob/main/Neuroimaging%20DICOM%20and%20NIfTI%20Data%20Curation%20Primer/neuroimaging-dicom-and-nifti-data-curation-primer.md\n",
    "# https://discovery.ucl.ac.uk/id/eprint/10146893/1/geometry_medim.pdf \n",
    "\n",
    "\n",
    "# test data\n",
    "# https://www.kaggle.com/datasets/aiocta/brats2023-part-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eed540dd-12d9-4f36-9da3-a367a4b341ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UC path \n",
    "# mmt_mlops_demos.cv.data\n",
    "# /Volumes/mmt_mlops_demos/cv/data/BraTS2021_00495/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d96c7f-77d7-4b73-8629-13a91d24c711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## to do -- convert some of the setup as a utils/config file  etc. \n",
    "\n",
    "!pip install nibabel -q\n",
    "!pip install scikit-learn -q\n",
    "!pip install tqdm -q\n",
    "!pip install split-folders -q\n",
    "!pip install torchinfo -q\n",
    "!pip install segmentation-models-pytorch-3d -q\n",
    "!pip install livelossplot -q\n",
    "!pip install torchmetrics -q\n",
    "!pip install tensorboard -q\n",
    "\n",
    "!pip install pycocotools\n",
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ad397d-4379-4789-a70f-5121cbedb132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09763bd8-4bc5-4c9e-a5c1-8ef0e88382c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Some of these might not be needed for dataprocessing"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import splitfolders\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import time\n",
    " \n",
    "from dataclasses import dataclass\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda import amp\n",
    " \n",
    "from torchmetrics import MeanMetric\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    " \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchinfo import summary\n",
    "import gc\n",
    " \n",
    "import segmentation_models_pytorch_3d as smp\n",
    " \n",
    "from livelossplot import PlotLosses\n",
    "from livelossplot.outputs import MatplotlibPlot, ExtremaPrinter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0680abd-3782-4d4e-9b29-c8ba0941cc4a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Download test data (BraTS2023) from Kaggle"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/aiocta/brats2023-part-1\n",
    "\n",
    "# Do ONCE\n",
    "# !pip install kaggle -q\n",
    "# !kaggle datasets download -d aiocta/brats2023-part-1 -p /Volumes/mmt_mlops_demos/cv/data/BraTS2023/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86dff492-bcf1-447b-aeb2-198e1da9c1f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Unzip BraTS2023 within UC Vols"
    }
   },
   "outputs": [],
   "source": [
    "# DO ONCE \n",
    "# !sudo apt install unzip\n",
    "# !unzip /Volumes/mmt_mlops_demos/cv/data/BraTS2023/brats2023-part-1.zip -d /Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023-Glioma/\n",
    "\n",
    "# !rm -rf <UD Vols path to>/brats2023-part-1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14cc6b40-b3ca-42d9-8ffe-6d655b50e194",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "maybe useful for later"
    }
   },
   "outputs": [],
   "source": [
    "# def seed_everything(SEED):\n",
    "#    np.random.seed(SEED)\n",
    "#    torch.manual_seed(SEED)\n",
    "#    torch.cuda.manual_seed_all(SEED)\n",
    "#    torch.backends.cudnn.deterministic = True\n",
    "#    torch.backends.cudnn.benchmark = False\n",
    " \n",
    " \n",
    "# def get_default_device():\n",
    "#    gpu_available = torch.cuda.is_available()\n",
    "#    return torch.device('cuda' if gpu_available else 'cpu'), gpu_available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16dbe55f-1ac2-4b7b-9034-72f4b83ff57e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Omitting because we are using YOLO instead"
    }
   },
   "outputs": [],
   "source": [
    "# @dataclass(frozen=True)\n",
    "# class TrainingConfig:\n",
    "#    BATCH_SIZE:      int = 5\n",
    "#    EPOCHS:          int = 100\n",
    "#    LEARNING_RATE: float = 1e-3\n",
    "#    CHECKPOINT_DIR:  str = os.path.join('model_checkpoint', '3D_UNet_Brats2023')\n",
    "#    NUM_WORKERS:     int = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00f1d1e-59bb-46c1-9e30-9fe3fec10ba3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "scaler = MinMaxScaler()"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    " \n",
    "DATASET_PATH = '/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023-Glioma/'\n",
    "print(\"Total Files: \", len(os.listdir(DATASET_PATH)))\n",
    "# Total Files:  625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df60930a-a60c-4a20-a380-5407a1785830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the NIfTI image\n",
    "sample_image_flair = nib.load(os.path.join(DATASET_PATH , \"BraTS-GLI-00000-000/BraTS-GLI-00000-000-t2f.nii\")).get_fdata()\n",
    "print(\"Original max value:\", sample_image_flair.max()) \n",
    "# Original max value: 2934.0\n",
    " \n",
    "# Reshape the 3D image to 2D for scaling\n",
    "sample_image_flair_flat = sample_image_flair.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b36c66c3-21cc-4c12-92f3-ca87b44dae15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_image_flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10cc3f8c-508c-4f3a-9d2b-29b9ae6b8baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_image_flair_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db57ce26-78c7-4632-b5f1-2cf4d0b02774",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "probably want to apply scaling  to the other image types?"
    }
   },
   "outputs": [],
   "source": [
    "# Apply scaling\n",
    "sample_image_flair_scaled = scaler.fit_transform(sample_image_flair_flat)\n",
    " \n",
    "# Reshape it back to the original 3D shape\n",
    "sample_image_flair_scaled = sample_image_flair_scaled.reshape(sample_image_flair.shape)\n",
    " \n",
    "print(\"Scaled max value:\", sample_image_flair_scaled.max())\n",
    "print(\"Shape of scaled Image: \", sample_image_flair_scaled.shape)\n",
    "\n",
    "# Scaled max value: 1.0\n",
    "# Shape of scaled Image:  (240, 240, 155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c776dab-c3b4-44d8-9c8b-8471b672b283",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "MASK info"
    }
   },
   "outputs": [],
   "source": [
    "sample_mask = nib.load(DATASET_PATH + \"BraTS-GLI-00000-000/BraTS-GLI-00000-000-seg.nii\").get_fdata()\n",
    "sample_mask = sample_mask.astype(np.uint8)  \n",
    " \n",
    "print(\"Unique class in the mask\", np.unique(sample_mask)) \n",
    "print(\"Shape of sample_mask: \", sample_mask.shape)\n",
    "\n",
    "# Unique class in the mask [0 1 2 3]\n",
    "# Shape of sample_mask:  (240, 240, 155) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b6d8e8c-af28-4478-91da-6e58bbf57408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_image_t1 = nib.load(DATASET_PATH + \"BraTS-GLI-00000-000/BraTS-GLI-00000-000-t1n.nii\").get_fdata()\n",
    "# sample_image_t1 = sample_image_t1.astype(np.uint8)  # values between 0 and 255 | NOT NEEDED?\n",
    "\n",
    "\n",
    "sample_image_t1ce = nib.load(DATASET_PATH + \"BraTS-GLI-00000-000/BraTS-GLI-00000-000-t1c.nii\").get_fdata()\n",
    "# sample_image_t1c = sample_image_t1c.astype(np.uint8)  # values between 0 and 255 | NOT NEEDED?\n",
    "\n",
    "\n",
    "sample_image_t2 = nib.load(DATASET_PATH + \"BraTS-GLI-00000-000/BraTS-GLI-00000-000-t2w.nii\").get_fdata()\n",
    "# sample_image_t2 = sample_image_t2.astype(np.uint8)  # values between 0 and 255 |  NOT NEEDED?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9febf5b-3ccb-4beb-9700-ac94fd2aed27",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test randint"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the range\n",
    "low = 50\n",
    "high = 90 #141\n",
    "\n",
    "# Generate a random integer between low (inclusive) and high (exclusive)\n",
    "rand_int = np.random.randint(low, high)\n",
    "print(f\"Random integer between {low} and {high}: {rand_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "509d7b1d-16e9-4756-bb43-6ac6c62b7f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# n_slice = random.randint(0, sample_mask.shape[2])  # random slice between 0 - 154\n",
    "n_slice = np.random.randint(low, high) #77\n",
    "print(\"n_slice: \", n_slice)\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    " \n",
    "plt.subplot(231)\n",
    "plt.imshow(sample_image_flair_scaled[:,:,n_slice], cmap='gray')\n",
    "plt.title('Image flair')\n",
    "plt.colorbar()\n",
    " \n",
    "plt.subplot(232)\n",
    "plt.imshow(sample_image_t1[:,:,n_slice], cmap = \"gray\")\n",
    "plt.title(\"Image t1\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.imshow(sample_image_t1ce[:,:,n_slice], cmap='gray')\n",
    "plt.title(\"Image t1ce\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.imshow(sample_image_t2[:,:,n_slice], cmap = 'gray')\n",
    "plt.title(\"Image t2\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.imshow(sample_mask[:,:,n_slice])\n",
    "plt.title(\"Seg Mask\")\n",
    "plt.colorbar()\n",
    " \n",
    "plt.subplot(236)\n",
    "plt.imshow(sample_mask[:,:,n_slice], cmap = 'gray')\n",
    "plt.title('Mask Gray')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c32ae0c-7596-4c6b-96c4-65c817a12d31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_x = np.stack(\n",
    "    [sample_image_flair_scaled, sample_image_t1ce, sample_image_t2], axis=3\n",
    ")  # along the last channel dimension.\n",
    "print(\"Shape of Combined x \", combined_x.shape)\n",
    "\n",
    "# Shape of Combined x  (240, 240, 155, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcf1b5dc-09a6-48ec-8cec-b9f12fa42ed8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## from https://learnopencv.com/3d-u-net-brats/#aioseo-dataset-preprocessing it was determined that the main mask ROIs are within [56:184, 56:184, 13:141] for the dataset -- something to derive from other datasets \n",
    "\n",
    "ROI_dims = [[56, 184], [56, 184], [13, 141]]\n",
    "ROI_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "627eae6e-0163-400e-9cc2-48f8dce2f938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# combined_x = combined_x[56:184, 56:184, 13:141]\n",
    "combined_x = combined_x[ROI_dims[0][0]:ROI_dims[0][1], \n",
    "                        ROI_dims[1][0]:ROI_dims[1][1],\n",
    "                        ROI_dims[2][0]:ROI_dims[2][1]\n",
    "                        ]\n",
    "print(\"Shape after cropping: \", combined_x.shape)\n",
    " \n",
    "# sample_mask_c = sample_mask[56:184,56:184, 13:141]\n",
    "sample_mask_c = sample_mask[ROI_dims[0][0]:ROI_dims[0][1], \n",
    "                            ROI_dims[1][0]:ROI_dims[1][1],\n",
    "                            ROI_dims[2][0]:ROI_dims[2][1]\n",
    "                            ]\n",
    "print(\"Mask shape after cropping: \", sample_mask_c.shape)\n",
    " \n",
    "#Shape after cropping:  (128, 128, 128, 3)\n",
    "#Mask shape after cropping:  (128, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b29f731b-b978-4643-a3da-96eb9c24e461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(list(range(56, 185)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93cbce52-e72a-4c59-9e73-066aea1348bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_mask[:,:,n_slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef13ef91-7f28-4cc5-8661-d158666a9d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_mask[:,:,n_slice].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "154c1141-51ef-4a74-af5b-73cc659ec318",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Curious what this 'looks' like hmm"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(#combined_x[:,:,n_slice-12],\n",
    "           combined_x[:,:,n_slice-ROI_dims[2][0]-1],\n",
    "          #   cmap = 'gray'\n",
    "          )\n",
    "plt.title(\"combined_x\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(#sample_mask_c[:,:,n_slice-12], \n",
    "           sample_mask_c[:,:,n_slice-ROI_dims[2][0]-1]\n",
    "          #  cmap = 'gray'\n",
    "          )\n",
    "plt.title(\"sample_mask_c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5273f73b-5486-4411-8179-d6e73fe80777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://www.synapse.org/Synapse:syn51156910/wiki/622351\n",
    "# Task: Tumor Sub-region Segmentation\n",
    "# The participants are called to address this task by using the provided clinically-acquired training data to develop their method and produce segmentation labels of the different glioma sub-regions. The sub-regions considered for evaluation are the \"enhancing tumor\" (ET), the \"tumor core\" (TC), and the \"whole tumor\" (WT) [see figure below]. The ET is described by areas that show hyper-intensity in T1Gd when compared to T1, but also when compared to “healthy” white matter in T1Gd. The TC describes the bulk of the tumor, which is what is typically resected. The TC entails the ET, as well as the necrotic (NCR) parts of the tumor. The appearance of NCR is typically hypo-intense in T1-Gd when compared to T1. The WT describes the complete extent of the disease, as it entails the TC and the peritumoral edematous/invaded tissue (ED), which is typically depicted by hyper-intense signal in FLAIR.\n",
    "\n",
    "# The provided segmentation labels have values of:\n",
    "\n",
    "# 1 for NCR\n",
    "# 2 for ED\n",
    "# 3 for ET\n",
    "# 0 for everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "590609f1-771d-418a-a1ec-58d28411f66a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for s in range(sample_mask_c.shape[0]):\n",
    "  print(s)\n",
    "  print(sample_mask_c[s,:,n_slice-ROI_dims[2][0]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "149bca19-37af-4ce9-9fc1-f226b498bde9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## one_hot encoding may not be necessary ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd4b971-53eb-4c16-a568-6e842a49dad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_mask_cat  = F.one_hot(torch.tensor(sample_mask_c, dtype = torch.long), num_classes = 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60883c2c-7289-41ce-81d9-16ac91e61fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_mask_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8652f7c4-7068-4eb6-afcc-bfe4f8743e0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for s in range(sample_mask_cat.shape[0]):\n",
    "#   print(s)\n",
    "#   print(sample_mask_cat[:,s,n_slice-ROI_dims[2][0]-1],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a712cb57-a353-4dd2-a206-4c1c6242f129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a40769a-045a-400e-b9f0-f1dd4125a692",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get list of images for processing"
    }
   },
   "outputs": [],
   "source": [
    "# t1ce_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*t1c.nii\"))\n",
    "# t2_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*t2w.nii\"))\n",
    "# flair_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*t2f.nii\"))\n",
    "# mask_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*seg.nii\"))\n",
    "\n",
    "\n",
    "# print(\"t1ce list: \", len(t1ce_list))\n",
    "# print(\"t2 list: \", len(t2_list))\n",
    "# print(\"flair list: \", len(flair_list))\n",
    "# print(\"Mask list: \", len(mask_list))\n",
    "\n",
    "# # t1ce list:  625\n",
    "# # t2 list:  625\n",
    "# # flair list:  625\n",
    "# # Mask list:  625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "489de3f2-4e4d-4ca8-b075-8d15d699adda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Save the lists to UC volume\n",
    "# dbutils.fs.put(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/t1ce_list.json\", json.dumps(t1ce_list), overwrite=True)\n",
    "# dbutils.fs.put(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/t2_list.json\", json.dumps(t2_list), overwrite=True)\n",
    "# dbutils.fs.put(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/flair_list.json\", json.dumps(flair_list), overwrite=True)\n",
    "# dbutils.fs.put(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/mask_list.json\", json.dumps(mask_list), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c90e776-6816-4794-9f8d-6c5d72a173f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON files from the UC volume using Unix commands\n",
    "t1ce_list_json = dbutils.fs.head(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/t1ce_list.json\", 1000000)\n",
    "t2_list_json = dbutils.fs.head(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/t2_list.json\", 1000000)\n",
    "flair_list_json = dbutils.fs.head(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/flair_list.json\", 1000000)\n",
    "mask_list_json = dbutils.fs.head(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/mask_list.json\", 1000000)\n",
    "\n",
    "# Parse the JSON strings into Python lists\n",
    "t1ce_list = json.loads(t1ce_list_json)\n",
    "t2_list = json.loads(t2_list_json)\n",
    "flair_list = json.loads(flair_list_json)\n",
    "mask_list = json.loads(mask_list_json)\n",
    "\n",
    "# Print the lengths of the lists\n",
    "print(\"t1ce list: \", len(t1ce_list))\n",
    "print(\"t2 list: \", len(t2_list))\n",
    "print(\"flair list: \", len(flair_list))\n",
    "print(\"Mask list: \", len(mask_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10077a89-3b0d-4335-97ec-42035e7afe97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to continue with preprocessing for normal pytorch process and then try to convert to coco "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c534c56-7709-4a7f-88f0-0894525fd548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## DATASET Preprocessing test to pytorch dataloader \n",
    "# -- we will need to see how to reformat to coco/yolo friendly format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7c188e-e866-4334-be47-603587e121e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "FOLDER defs"
    }
   },
   "outputs": [],
   "source": [
    "# '/'.join(f\"{DATASET_PATH}\".split(\"/\")[:-2])\n",
    "UCV_folderpath =  \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/\"\n",
    "UCV_subfoldername = \"BraTS2023_Preprocessed\"\n",
    "# UCV_subfoldername = \"BraTS2023_Preprocessed_v2\" ## without one-hot-mask encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b12efe-bde2-4be0-b476-8e79bd51ea8a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "maybe there's a way to distribute this via spark udf OR streaming?"
    }
   },
   "outputs": [],
   "source": [
    "## Do Once\n",
    "\n",
    "for idx in tqdm(\n",
    "    range(len(t2_list)), desc=\"Preparing to stack, crop and save\", unit=\"file\"\n",
    "):\n",
    "    temp_image_t1ce = nib.load(t1ce_list[idx]).get_fdata()\n",
    "    temp_image_t1ce = scaler.fit_transform(\n",
    "        temp_image_t1ce.reshape(-1, temp_image_t1ce.shape[-1])\n",
    "    ).reshape(temp_image_t1ce.shape)\n",
    " \n",
    "    temp_image_t2 = nib.load(t2_list[idx]).get_fdata()\n",
    "    temp_image_t2 = scaler.fit_transform(\n",
    "        temp_image_t2.reshape(-1, temp_image_t2.shape[-1])\n",
    "    ).reshape(temp_image_t2.shape)\n",
    " \n",
    "    temp_image_flair = nib.load(flair_list[idx]).get_fdata()\n",
    "    temp_image_flair = scaler.fit_transform(\n",
    "        temp_image_flair.reshape(-1, temp_image_flair.shape[-1])\n",
    "    ).reshape(temp_image_flair.shape)\n",
    " \n",
    "    temp_mask = nib.load(mask_list[idx]).get_fdata()\n",
    " \n",
    "    temp_combined_images = np.stack(\n",
    "        [temp_image_flair, temp_image_t1ce, temp_image_t2], axis=3\n",
    "    )\n",
    " \n",
    "    temp_combined_images = temp_combined_images[56:184, 56:184, 13:141]\n",
    "    temp_mask = temp_mask[56:184, 56:184, 13:141]\n",
    " \n",
    "    val, counts = np.unique(temp_mask, return_counts=True)\n",
    " \n",
    "    # If a volume has less than 1% of mask, we simply ignore to reduce computation\n",
    "    if (1 - (counts[0] / counts.sum())) > 0.01:\n",
    "        #         print(\"Saving Processed Images and Masks\")\n",
    "        \n",
    "        ## omit the one_hot encoding ?\n",
    "        if UCV_subfoldername != \"BraTS2023_Preprocessed_v2\":\n",
    "            temp_mask = F.one_hot(torch.tensor(temp_mask, dtype=torch.long), num_classes=4)\n",
    "        \n",
    "        os.makedirs(f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/images\", exist_ok=True)\n",
    "        os.makedirs(f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/masks\", \n",
    "                    exist_ok=True)\n",
    " \n",
    "        np.save(\n",
    "            f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/images/image_\"\n",
    "            + str(idx)\n",
    "            + \".npy\",\n",
    "            temp_combined_images,\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/masks/mask_\"\n",
    "            + str(idx)\n",
    "            + \".npy\",\n",
    "            temp_mask,\n",
    "        )\n",
    " \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7a64bb6-c5da-4059-9c93-c11f07ee3eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "022f2f0e-15bb-40c7-aa4d-54e10348bc77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# compare tqdm vs vectorized pandasUDF for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e3901933-b169-4dee-9309-e407b7462871",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Vectorize processing"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import nibabel as nib\n",
    "# import os\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from typing import List\n",
    "\n",
    "# # Define the UDF\n",
    "# @pandas_udf(\"string\", PandasUDFType.SCALAR)\n",
    "# def process_images(t1ce_path: pd.Series, \n",
    "#                    t2_path: pd.Series, \n",
    "#                    flair_path: pd.Series, \n",
    "#                    mask_path: pd.Series, \n",
    "#                    idx: pd.Series\n",
    "#                    ) -> pd.Series:\n",
    "#     results: List[str] = []\n",
    "#     for i in range(len(t1ce_path)):\n",
    "#         temp_image_t1ce: np.ndarray = nib.load(t1ce_path[i]).get_fdata()\n",
    "#         temp_image_t1ce = scaler.fit_transform(\n",
    "#             temp_image_t1ce.reshape(-1, temp_image_t1ce.shape[-1])\n",
    "#         ).reshape(temp_image_t1ce.shape)\n",
    "\n",
    "#         temp_image_t2: np.ndarray = nib.load(t2_path[i]).get_fdata()\n",
    "#         temp_image_t2 = scaler.fit_transform(\n",
    "#             temp_image_t2.reshape(-1, temp_image_t2.shape[-1])\n",
    "#         ).reshape(temp_image_t2.shape)\n",
    "\n",
    "#         temp_image_flair: np.ndarray = nib.load(flair_path[i]).get_fdata()\n",
    "#         temp_image_flair = scaler.fit_transform(\n",
    "#             temp_image_flair.reshape(-1, temp_image_flair.shape[-1])\n",
    "#         ).reshape(temp_image_flair.shape)\n",
    "\n",
    "#         temp_mask: np.ndarray = nib.load(mask_path[i]).get_fdata()\n",
    "\n",
    "#         temp_combined_images: np.ndarray = np.stack(\n",
    "#             [temp_image_flair, temp_image_t1ce, temp_image_t2], axis=3\n",
    "#         )\n",
    "\n",
    "#         temp_combined_images = temp_combined_images[56:184, 56:184, 13:141]\n",
    "#         temp_mask = temp_mask[56:184, 56:184, 13:141]\n",
    "\n",
    "#         val, counts = np.unique(temp_mask, return_counts=True)\n",
    "\n",
    "#         # If a volume has less than 1% of mask, we simply ignore to reduce computation\n",
    "#         if (1 - (counts[0] / counts.sum())) > 0.01:\n",
    "#             if UCV_subfoldername != \"BraTS2023_Preprocessed_v2\":\n",
    "#                 temp_mask = F.one_hot(torch.tensor(temp_mask, dtype=torch.long), num_classes=4).numpy()\n",
    "            \n",
    "#             images_dir: str = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/images\"\n",
    "#             masks_dir: str = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/masks\"\n",
    "#             os.makedirs(images_dir, exist_ok=True)\n",
    "#             os.makedirs(masks_dir, exist_ok=True)\n",
    "\n",
    "#             np.save(\n",
    "#                 f\"{images_dir}/image_\" + str(idx[i]) + \".npy\",\n",
    "#                 temp_combined_images,\n",
    "#             )\n",
    "#             np.save(\n",
    "#                 f\"{masks_dir}/mask_\" + str(idx[i]) + \".npy\",\n",
    "#                 temp_mask,\n",
    "#             )\n",
    "#             results.append(\"Processed\")\n",
    "#         else:\n",
    "#             results.append(\"Skipped\")\n",
    "#     return pd.Series(results)\n",
    "\n",
    "# # Create a Spark DataFrame\n",
    "# data = list(zip(t1ce_list, t2_list, flair_list, mask_list, range(len(t1ce_list))))\n",
    "# columns = [\"t1ce_path\", \"t2_path\", \"flair_path\", \"mask_path\", \"idx\"]\n",
    "# df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# # Apply the UDF\n",
    "# df = df.withColumn(\"result\", process_images(\"t1ce_path\", \"t2_path\", \"flair_path\", \"mask_path\", \"idx\"))\n",
    "\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd131a58-4ff3-4fb5-b835-c2e46db19e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images_folder = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/images\"\n",
    "print(len(os.listdir(images_folder)))\n",
    " \n",
    "masks_folder = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/masks\"\n",
    "print(len(os.listdir(masks_folder)))\n",
    "\n",
    "# Images: 575\n",
    "# Masks: 575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ee35ca-ce8c-4603-a0a1-9a5d6ba77af3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "split into train / val"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "input_folder = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/\"\n",
    " \n",
    "output_folder = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_128/\"\n",
    "\n",
    "## do once \n",
    "splitfolders.ratio(\n",
    "    input_folder, output_folder, seed=42, ratio=(0.75, 0.25), group_prefix=None\n",
    "    # input_folder, output_folder, seed=42, ratio=(0.7, 0.2, 0.1), group_prefix=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c529ea6-b35d-4c1e-8d25-f0630fe04926",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "NOT deleting files"
    }
   },
   "outputs": [],
   "source": [
    "# if os.path.exists(input_folder):\n",
    "#     shutil.rmtree(input_folder)\n",
    "#     print(f\"{input_folder} is removed\")\n",
    "# else:\n",
    "#     print(f\"{input_folder} doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3ba008d-830c-4921-a54d-f356a46de86a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "define a custom data loader?"
    }
   },
   "outputs": [],
   "source": [
    "class BraTSDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, normalization=True):\n",
    "        super().__init__()\n",
    " \n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.img_list = sorted(\n",
    "            os.listdir(img_dir)\n",
    "        )  # Ensure sorting to match images and masks\n",
    "        self.mask_list = sorted(os.listdir(mask_dir))\n",
    "        self.normalization = normalization\n",
    " \n",
    "        # If normalization is True, set up a normalization transform\n",
    "        if self.normalization:\n",
    "            self.normalizer = transforms.Normalize(\n",
    "                mean=[0.5], std=[0.5]\n",
    "            )  # Adjust mean and std based on your data\n",
    " \n",
    "    def load_file(self, filepath):\n",
    "        return np.load(filepath)\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "       image_path = os.path.join(self.img_dir, self.img_list[idx])\n",
    "       mask_path = os.path.join(self.mask_dir, self.mask_list[idx])\n",
    "       # Load the image and mask\n",
    "       image = self.load_file(image_path)\n",
    "       mask = self.load_file(mask_path)\n",
    " \n",
    "       # Convert to torch tensors and permute axes to C, D, H, W format (needed for 3D models)\n",
    "       image = torch.from_numpy(image).permute(3, 2, 0, 1)  # Shape: C, D, H, W\n",
    "    \n",
    "       mask = torch.from_numpy(mask).permute(3, 2, 0, 1)  # Shape: C, D, H, W\n",
    "       \n",
    "       # mask = torch.from_numpy(mask).permute(2, 0, 1)  # Shape: D, H, W\n",
    "       # mask = torch.from_numpy(mask).permute(2, 0, 1).unsqueeze(0)  # Shape: 1, D, H, W (added channel dimension)\n",
    "       \n",
    "       # Normalize the image if normalization is enabled\n",
    "       if self.normalization:\n",
    "           image = self.normalizer(image)\n",
    "       \n",
    "       return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "709e2f17-ba9a-438b-a421-010d21fda061",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LOAD data into dataloader"
    }
   },
   "outputs": [],
   "source": [
    "train_img_dir = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_128/train/images\"\n",
    "train_mask_dir = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_128/train/masks\"\n",
    " \n",
    "val_img_dir = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_128/val/images\"\n",
    "val_mask_dir = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_128/val/masks\"\n",
    " \n",
    "val_img_list = os.listdir(val_img_dir)\n",
    "val_mask_list = os.listdir(val_mask_dir)\n",
    " \n",
    "# Initialize datasets with normalization only\n",
    "train_dataset = BraTSDataset(train_img_dir, train_mask_dir, normalization=True)\n",
    "val_dataset = BraTSDataset(val_img_dir, val_mask_dir, normalization=True)\n",
    " \n",
    "# Print dataset statistics\n",
    "print(\"Total Training Samples: \", len(train_dataset))\n",
    "print(\"Total Val Samples: \", len(val_dataset))\n",
    "\n",
    "#Total Training Samples:  431\n",
    "#Total Val Samples:  144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1b678a32-e98e-4db0-8823-6ece6375be25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check sizes"
    }
   },
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=4)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False, num_workers=4)\n",
    "\n",
    "# # Sanity Check\n",
    "# images, masks = next(iter(train_loader))\n",
    "# print(f\"Train Image batch shape: {images.shape}\")\n",
    "# print(f\"Train Mask batch shape: {masks.shape}\")\n",
    "\n",
    "# # Train Image batch shape: torch.Size([5, 3, 128, 128, 128])\n",
    "# # Train Mask batch shape: torch.Size([5, 4, 128, 128, 128])\n",
    "\n",
    "\n",
    "# # Train Image batch shape: torch.Size([5, 3, 128, 128, 128])\n",
    "# # Train Mask batch shape: torch.Size([5, 1, 128, 128, 128])\n",
    "\n",
    "# # Train Image batch shape: torch.Size([5, 3, 128, 128, 128])\n",
    "# # Train Mask batch shape: torch.Size([5, 128, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78731370-7549-4515-b7bd-38c24fc242fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## redo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf0b8b3-9f8f-4867-8409-9c77081f1d09",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "with conditional"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False, num_workers=4)\n",
    "\n",
    "# Sanity Check\n",
    "images, masks = next(iter(train_loader))\n",
    "print(f\"Train Image batch shape: {images.shape}\")\n",
    "print(f\"Train Mask batch shape: {masks.shape}\")\n",
    "\n",
    "# # Adjust mask dimensions if necessary\n",
    "# if masks.dim() == 3:\n",
    "#     masks = masks.unsqueeze(0)  # Add a channel dimension if missing\n",
    "# elif masks.dim() == 4:\n",
    "#     masks = masks.permute(3, 2, 0, 1)  # Shape: C, D, H, W\n",
    "\n",
    "\n",
    "# Train Image batch shape: torch.Size([5, 3, 128, 128, 128])\n",
    "# Train Mask batch shape: torch.Size([5, 4, 128, 128, 128])\n",
    "\n",
    "# Train Image batch shape: torch.Size([5, 3, 128, 128, 128])\n",
    "# Train Mask batch shape: torch.Size([5, 1, 128, 128, 128])\n",
    "\n",
    "# Train Image batch shape: torch.Size([5, 3, 128, 128, 128])\n",
    "# Train Mask batch shape: torch.Size([5, 128, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47409b53-0b04-4d77-8899-d1f3499b7a33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(128):\n",
    "  print(i)\n",
    "  print(masks[3,0,64,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75257352-9cc8-4058-a820-9d7bc7e104aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# masks.shape \n",
    "# torch.Size([5, 4, 128, 128, 128])\n",
    "\n",
    "# torch.argmax(masks, dim=1).shape\n",
    "# torch.Size([5, 128, 128, 128])\n",
    "\n",
    "for i in range(128):\n",
    "  print(i)\n",
    "  print(torch.argmax(masks, dim=1)[1,64,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2ce594-b450-44c6-88b7-869e3984a48f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def visualize_slices(images, masks, num_slices=20):\n",
    "    batch_size = images.shape[0]\n",
    " \n",
    "    # if torch.Size([5, 4, 128, 128, 128])\n",
    "    masks = torch.argmax(masks, dim=1)  # along the channel/class dim -- predicted value of each class for that dim (class)\n",
    "    ## no longer needed when all is already \"combined\" in the mask\n",
    " \n",
    "    for i in range(min(num_slices, batch_size)):\n",
    "        fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    " \n",
    "        middle_slice = images.shape[2] // 2\n",
    "        ax[0].imshow(images[i, 0, middle_slice, :, :], cmap=\"gray\")\n",
    "        ax[1].imshow(images[i, 1, middle_slice, :, :], cmap=\"gray\")\n",
    "        ax[2].imshow(images[i, 2, middle_slice, :, :], cmap=\"gray\")\n",
    "        ax[3].imshow(masks[i, middle_slice, :, :], cmap=\"viridis\")\n",
    "        ax[4].imshow(masks[i, middle_slice, :, :], cmap=\"gray\")\n",
    "\n",
    "        # ax[3].imshow(masks[i][middle_slice, :, :], cmap=\"viridis\")\n",
    "        # ax[4].imshow(masks[i][middle_slice, :, :], cmap=\"gray\")\n",
    "\n",
    "        ax[0].set_title(\"T1ce\")\n",
    "        ax[1].set_title(\"FLAIR\")\n",
    "        ax[2].set_title(\"T2\")\n",
    "        ax[3].set_title(\"Seg Mask\")\n",
    "        ax[4].set_title(\"Mask - Gray\")\n",
    " \n",
    "        plt.show()\n",
    " \n",
    " \n",
    "visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f890ddcb-9d96-489e-81fb-2bdb4a6567f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images, masks = next(iter(train_loader))\n",
    "visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e5583013-de48-4ca4-b41e-2d03212be32b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def visualize_slices(images, masks, num_slices=20):\n",
    "#     batch_size = images.shape[0]\n",
    " \n",
    "#     ## if torch.Size([5, 4, 128, 128, 128])\n",
    "#     masks = torch.argmax(masks, dim=1)  # along the channel/class dim\n",
    " \n",
    "#     for i in range(min(num_slices, batch_size)):\n",
    "#         fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    " \n",
    "#         middle_slice = images.shape[2] // 2\n",
    "#         ax[0].imshow(images[i, 0, middle_slice, :, :], cmap=\"gray\")\n",
    "#         ax[1].imshow(images[i, 1, middle_slice, :, :], cmap=\"gray\")\n",
    "#         ax[2].imshow(images[i, 2, middle_slice, :, :], cmap=\"gray\")\n",
    "                \n",
    "#         ax[3].imshow(masks[i, middle_slice, :, :], cmap=\"viridis\")\n",
    "#         ax[4].imshow(masks[i, middle_slice, :, :], cmap=\"gray\")\n",
    "#         # ax[3].imshow(masks[i][middle_slice, :, :], cmap=\"viridis\")\n",
    "#         # ax[4].imshow(masks[i][middle_slice, :, :], cmap=\"gray\")\n",
    "\n",
    "        \n",
    "#         ax[0].set_title(\"T1ce\")\n",
    "#         ax[1].set_title(\"FLAIR\")\n",
    "#         ax[2].set_title(\"T2\")\n",
    "#         ax[3].set_title(\"Seg Mask\")\n",
    "#         ax[4].set_title(\"Mask - Gray\")\n",
    " \n",
    "#         plt.show()\n",
    " \n",
    " \n",
    "# visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ee7d497-cba0-466b-8957-8548c1ef397d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "middle_slice = images.shape[2] // 2\n",
    "middle_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fbc7490-7b4f-4040-9499-6a51b456fe73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming middle_slice is already defined\n",
    "middle_slice = images.shape[2] // 2\n",
    "\n",
    "# Convert the mask to a 2D array\n",
    "mask_2d = masks[3, middle_slice, :, :]\n",
    "\n",
    "# Plot the 2D mask\n",
    "plt.imshow(mask_2d, cmap=\"viridis\")\n",
    "plt.title(f\"Mask for middle slice {middle_slice}\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9d5280-ca2f-45d4-88ca-f14a39c49c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# images, masks = next(iter(train_loader))\n",
    "# visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2de02897-f985-47f5-a2cc-a6103d6544a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc36be49-740c-40c9-8fbf-2026d8831207",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Viz"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_slices(images, masks, num_slices=20):\n",
    "    batch_size = images.shape[0]\n",
    " \n",
    "    masks = torch.argmax(masks, dim=1)  # along the channel/class dim\n",
    " \n",
    "    for i in range(min(num_slices, batch_size)):\n",
    "        fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    " \n",
    "        middle_slice = images.shape[2] // 2\n",
    "        ax[0].imshow(images[i, 0, middle_slice, :, :], cmap=\"gray\")\n",
    "        ax[1].imshow(images[i, 1, middle_slice, :, :], cmap=\"gray\")\n",
    "        ax[2].imshow(images[i, 2, middle_slice, :, :], cmap=\"gray\")\n",
    "        ax[3].imshow(masks[i, middle_slice, :, :], cmap=\"viridis\")\n",
    "        ax[4].imshow(masks[i, middle_slice, :, :], cmap=\"gray\")\n",
    " \n",
    "        ax[0].set_title(\"T1ce\")\n",
    "        ax[1].set_title(\"FLAIR\")\n",
    "        ax[2].set_title(\"T2\")\n",
    "        ax[3].set_title(\"Seg Mask\")\n",
    "        ax[4].set_title(\"Mask - Gray\")\n",
    " \n",
    "        plt.show()\n",
    " \n",
    " \n",
    "visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94dee085-aaf3-4d5b-894e-faaa198c75d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bade9fc-8144-49a6-bf2b-182ce1d74fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e7de203-8c09-456e-934d-f616a8c09d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pycocotools\n",
    "!pip install opencv-python-headless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e8879d6-fd5b-4e4a-961b-918ad602f114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab8e9181-febf-4491-b0a5-c2c113950d5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# from pycocotools import mask as maskUtils\n",
    "\n",
    "# def convert_to_coco_format(images, masks, categories):\n",
    "#     coco_format = {\n",
    "#         \"images\": [],\n",
    "#         \"annotations\": [],\n",
    "#         \"categories\": []\n",
    "#     }\n",
    "\n",
    "#     annotation_id = 1\n",
    "\n",
    "#     for i in range(images.shape[0]):\n",
    "#         image_info = {\n",
    "#             \"id\": i,\n",
    "#             \"width\": images.shape[3],\n",
    "#             \"height\": images.shape[4],\n",
    "#             \"file_name\": f\"image_{i}.png\"\n",
    "#         }\n",
    "#         coco_format[\"images\"].append(image_info)\n",
    "\n",
    "#         for category_id in range(masks.shape[1]):\n",
    "#             mask = masks[i, category_id, :, :].numpy().astype(np.uint8)\n",
    "#             mask = np.asfortranarray(mask)\n",
    "#             rle = maskUtils.encode(mask)\n",
    "#             if isinstance(rle, list):\n",
    "#                 rle = rle[0]  # Take the first element if rle is a list\n",
    "#             rle['counts'] = rle['counts'].decode('utf-8')  # Decode bytes to string\n",
    "#             area = maskUtils.area(rle)\n",
    "#             bbox = maskUtils.toBbox(rle)\n",
    "\n",
    "#             annotation_info = {\n",
    "#                 \"id\": annotation_id,\n",
    "#                 \"image_id\": i,\n",
    "#                 \"category_id\": category_id,\n",
    "#                 \"segmentation\": rle,\n",
    "#                 \"area\": area.tolist(),\n",
    "#                 \"bbox\": bbox.tolist(),\n",
    "#                 \"iscrowd\": 0\n",
    "#             }\n",
    "#             coco_format[\"annotations\"].append(annotation_info)\n",
    "#             annotation_id += 1\n",
    "\n",
    "#     for category_id, category_name in enumerate(categories):\n",
    "#         category_info = {\n",
    "#             \"id\": category_id,\n",
    "#             \"name\": category_name\n",
    "#         }\n",
    "#         coco_format[\"categories\"].append(category_info)\n",
    "\n",
    "#     return coco_format\n",
    "\n",
    "# # Example usage\n",
    "# # categories = [\"category1\", \"category2\", \"category3\", \"category4\"]\n",
    "\n",
    "# # 1 for NCR\n",
    "# # 2 for ED\n",
    "# # 3 for ET\n",
    "# # 0 for everything else.\n",
    "\n",
    "# categories = [\"nonT\", \"NCR\", \"ED\", \"ET\"]\n",
    "\n",
    "# coco_format = convert_to_coco_format(images, masks, categories)\n",
    "\n",
    "# # Save to JSON file\n",
    "# # with open(\"coco_annotations.json\", \"w\") as f:\n",
    "# #     json.dump(coco_format, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2225f9ca-d184-414c-9c61-8b34853499e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# coco_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f65e373-09a3-46ed-b46c-f3f992b85733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "coco_format.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfcb208b-9b6a-42b0-a90b-5a1d30b764a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# coco_format['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "848392f7-35b1-4952-a93a-4b8e0292c628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(coco_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27c06820-3b2f-4873-a263-74fcf9777488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ee3fa7d-cf16-49be-a965-24585d7df4cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def create_coco_json(images, masks, categories, output_file):\n",
    "    coco_data = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": categories\n",
    "    }\n",
    "    \n",
    "    annotation_id = 1\n",
    "    for image_id, (image_path, mask_path) in enumerate(zip(images, masks)):\n",
    "        image = Image.open(f\"/dbfs{image_path}\")\n",
    "        width, height = image.size\n",
    "        \n",
    "        coco_data[\"images\"].append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": os.path.basename(image_path),\n",
    "            \"width\": width,\n",
    "            \"height\": height\n",
    "        })\n",
    "        \n",
    "        mask = np.array(Image.open(f\"/dbfs{mask_path}\"))\n",
    "        for category in categories:\n",
    "            category_id = category[\"id\"]\n",
    "            binary_mask = (mask == category_id).astype(np.uint8)\n",
    "            if binary_mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            # Find bounding box\n",
    "            pos = np.where(binary_mask)\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            bbox = [xmin, ymin, xmax - xmin, ymax - ymin]\n",
    "            \n",
    "            # Create annotation\n",
    "            coco_data[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": bbox,\n",
    "                \"area\": int(binary_mask.sum()),\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "    \n",
    "    with open(f\"/dbfs{output_file}\", \"w\") as f:\n",
    "        json.dump(coco_data, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7e034d-ec81-4ef9-8a2c-3bc3465c299d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to the JSON file in DBFS\n",
    "json_file_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed/coco_annotations.json\"\n",
    "\n",
    "# Read the JSON file\n",
    "with open(json_file_path, 'r') as f:\n",
    "    coco_annotations = json.load(f)\n",
    "\n",
    "# Display the content\n",
    "print(coco_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ec137af-2368-4d1b-8866-398233a512a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "306391fa-e374-4982-9955-179848f35a1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "def create_coco_json(images, masks, categories, output_file):\n",
    "    coco_data = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": categories\n",
    "    }\n",
    "    \n",
    "    annotation_id = 1\n",
    "    for image_id, (image_path, mask_path) in enumerate(zip(images, masks)):\n",
    "        print(f\"Processing image: {image_path}, mask: {mask_path}\")\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image file not found: {image_path}\")\n",
    "            continue\n",
    "        if not os.path.exists(mask_path):\n",
    "            print(f\"Mask file not found: {mask_path}\")\n",
    "            continue\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        width, height = image.size\n",
    "\n",
    "        coco_data[\"images\"].append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": os.path.basename(image_path),\n",
    "            \"width\": width,\n",
    "            \"height\": height\n",
    "        })\n",
    "\n",
    "        mask = Image.open(mask_path)\n",
    "        # Assuming masks are binary images with 0 and 255 values\n",
    "        mask_data = mask.getdata()\n",
    "        mask_data = [1 if pixel > 0 else 0 for pixel in mask_data]\n",
    "\n",
    "        coco_data[\"annotations\"].append({\n",
    "            \"id\": annotation_id,\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": 1,  # Assuming a single category for simplicity\n",
    "            \"segmentation\": mask_data,\n",
    "            \"area\": sum(mask_data),\n",
    "            \"bbox\": [0, 0, width, height],\n",
    "            \"iscrowd\": 0\n",
    "        })\n",
    "        annotation_id += 1\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(coco_data, f)\n",
    "\n",
    "# Example usage\n",
    "images = [\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed/input_data_128/train/images/image1.jpg\", \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed/input_data_128/train/images/image2.jpg\"]\n",
    "masks = [\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed/input_data_128/train/images/mask1.png\", \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed/input_data_128/train/images/mask2.png\"]\n",
    "categories = [{\"id\": 1, \"name\": \"category1\"}, {\"id\": 2, \"name\": \"category2\"}]\n",
    "output_file = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed/coco_annotations.json\"\n",
    "create_coco_json(images, masks, categories, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb2550d-ba17-4d8b-be98-321f8e7c63e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to the JSON file in DBFS\n",
    "json_file_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed/coco_annotations.json\"\n",
    "\n",
    "# Read the JSON file\n",
    "with open(json_file_path, 'r') as f:\n",
    "    coco_annotations = json.load(f)\n",
    "\n",
    "# Display the content\n",
    "print(coco_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4a94c07-aaa2-4df2-bf11-ebaca4ac7102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d63ea2d-d268-4201-83e8-5f357746cd35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d08181-cd3f-4dba-8e00-e2ca16f9fa44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def create_yolo_files(images, masks, categories, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    category_map = {category[\"id\"]: idx for idx, category in enumerate(categories)}\n",
    "    \n",
    "    for image_path, mask_path in zip(images, masks):\n",
    "        image = Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        \n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        annotations = []\n",
    "        \n",
    "        for category in categories:\n",
    "            category_id = category[\"id\"]\n",
    "            binary_mask = (mask == category_id).astype(np.uint8)\n",
    "            if binary_mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            # Find bounding box\n",
    "            pos = np.where(binary_mask)\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            \n",
    "            # Convert to YOLO format\n",
    "            center_x = (xmin + xmax) / 2 / width\n",
    "            center_y = (ymin + ymax) / 2 / height\n",
    "            bbox_width = (xmax - xmin) / width\n",
    "            bbox_height = (ymax - ymin) / height\n",
    "            \n",
    "            annotations.append(f\"{category_map[category_id]} {center_x} {center_y} {bbox_width} {bbox_height}\")\n",
    "        \n",
    "        # Save annotations to file\n",
    "        annotation_file = os.path.join(output_dir, os.path.splitext(os.path.basename(image_path))[0] + \".txt\")\n",
    "        with open(annotation_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(annotations))\n",
    "\n",
    "# Example usage\n",
    "images = [\"path/to/image1.jpg\", \"path/to/image2.jpg\"]\n",
    "masks = [\"path/to/mask1.png\", \"path/to/mask2.png\"]\n",
    "categories = [{\"id\": 1, \"name\": \"category1\"}, {\"id\": 2, \"name\": \"category2\"}]\n",
    "output_dir = \"yolo_annotations\"\n",
    "create_yolo_files(images, masks, categories, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3c4f9e2-84e6-4e74-9677-82cf14bb5315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "read_explore_imageNlabels_nii_data_2023_v1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
