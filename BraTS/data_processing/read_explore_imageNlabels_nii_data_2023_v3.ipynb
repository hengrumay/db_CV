{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01d03fa5-5527-4ed3-b0b4-f3f11471db26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## TODO -- will need additional image preprocessing -- 2D coronal slices + corresponding masks \n",
    "\n",
    "## test using non onehot encoded dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093e11ea-7db3-43da-b83e-d7ab85a5a23c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Refs/Notes"
    }
   },
   "outputs": [],
   "source": [
    "# refs \n",
    "# https://learnopencv.com/3d-u-net-brats/#aioseo-dataset-preprocessing (WRT BraTS dataset)\n",
    "# https://github.com/spmallick/learnopencv/tree/master/Training_3D_U-Net_Brain_Tumor_Seg\n",
    "\n",
    "# https://nipy.org/nibabel/nifti_images.html\n",
    "\n",
    "## NIFTI (brain imaging related but not everyone uses it; DICOM may be preferred)\n",
    "# https://github.com/DataCurationNetwork/data-primers/blob/main/Neuroimaging%20DICOM%20and%20NIfTI%20Data%20Curation%20Primer/neuroimaging-dicom-and-nifti-data-curation-primer.md\n",
    "# https://discovery.ucl.ac.uk/id/eprint/10146893/1/geometry_medim.pdf \n",
    "\n",
    "\n",
    "# test data\n",
    "# https://www.kaggle.com/datasets/aiocta/brats2023-part-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eed540dd-12d9-4f36-9da3-a367a4b341ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UC path \n",
    "# mmt_mlops_demos.cv.data\n",
    "# /Volumes/mmt_mlops_demos/cv/data/BraTS2021_00495/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d96c7f-77d7-4b73-8629-13a91d24c711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## to do -- convert some of the setup as a utils/config file  etc. \n",
    "\n",
    "!pip install nibabel -q\n",
    "!pip install scikit-learn -q\n",
    "!pip install tqdm -q\n",
    "!pip install split-folders -q\n",
    "!pip install torchinfo -q\n",
    "!pip install segmentation-models-pytorch-3d -q\n",
    "!pip install livelossplot -q\n",
    "!pip install torchmetrics -q\n",
    "!pip install tensorboard -q\n",
    "\n",
    "!pip install pycocotools\n",
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ad397d-4379-4789-a70f-5121cbedb132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09763bd8-4bc5-4c9e-a5c1-8ef0e88382c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Some of these might not be needed for dataprocessing"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import splitfolders\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import time\n",
    " \n",
    "from dataclasses import dataclass\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda import amp\n",
    " \n",
    "from torchmetrics import MeanMetric\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    " \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchinfo import summary\n",
    "import gc\n",
    " \n",
    "import segmentation_models_pytorch_3d as smp\n",
    " \n",
    "from livelossplot import PlotLosses\n",
    "from livelossplot.outputs import MatplotlibPlot, ExtremaPrinter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0680abd-3782-4d4e-9b29-c8ba0941cc4a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Download test data (BraTS2023) from Kaggle"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/aiocta/brats2023-part-1\n",
    "\n",
    "# Do ONCE\n",
    "# !pip install kaggle -q\n",
    "# !kaggle datasets download -d aiocta/brats2023-part-1 -p /Volumes/mmt_mlops_demos/cv/data/BraTS2023/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86dff492-bcf1-447b-aeb2-198e1da9c1f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Unzip BraTS2023 within UC Vols"
    }
   },
   "outputs": [],
   "source": [
    "# DO ONCE \n",
    "# !sudo apt install unzip\n",
    "# !unzip /Volumes/mmt_mlops_demos/cv/data/BraTS2023/brats2023-part-1.zip -d /Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023-Glioma/\n",
    "\n",
    "# !rm -rf <UD Vols path to>/brats2023-part-1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52a0fa74-2e85-49d6-b029-766c67dd5f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc05fde7-0434-45fc-ae94-f5a05c9f4a9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5273f73b-5486-4411-8179-d6e73fe80777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://www.synapse.org/Synapse:syn51156910/wiki/622351\n",
    "# Task: Tumor Sub-region Segmentation\n",
    "# The participants are called to address this task by using the provided clinically-acquired training data to develop their method and produce segmentation labels of the different glioma sub-regions. The sub-regions considered for evaluation are the \"enhancing tumor\" (ET), the \"tumor core\" (TC), and the \"whole tumor\" (WT) [see figure below]. The ET is described by areas that show hyper-intensity in T1Gd when compared to T1, but also when compared to “healthy” white matter in T1Gd. The TC describes the bulk of the tumor, which is what is typically resected. The TC entails the ET, as well as the necrotic (NCR) parts of the tumor. The appearance of NCR is typically hypo-intense in T1-Gd when compared to T1. The WT describes the complete extent of the disease, as it entails the TC and the peritumoral edematous/invaded tissue (ED), which is typically depicted by hyper-intense signal in FLAIR.\n",
    "\n",
    "# The provided segmentation labels have values of:\n",
    "\n",
    "# 1 for NCR\n",
    "# 2 for ED\n",
    "# 3 for ET\n",
    "# 0 for everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "590609f1-771d-418a-a1ec-58d28411f66a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for s in range(sample_mask_c.shape[0]):\n",
    "  print(s)\n",
    "  print(sample_mask_c[s,:,n_slice-ROI_dims[2][0]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86b27ef2-4652-46de-a7c5-500c511066e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_type= 't2f_128'\n",
    "output_dir = f'/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_2D/{output_type}/'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6799a316-b6d3-46d8-b8b0-b9236779173b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f93665a9-2259-44b9-b2d8-4786fda1889e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "3D to 2D"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image\n",
    "\n",
    "# Define the paths\n",
    "input_dir = '/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023-Glioma/'\n",
    "\n",
    "# scan_type = 'tf2'\n",
    "scan_type = 'seg'\n",
    "\n",
    "# output_type= 't2f_128'\n",
    "output_type= f'{scan_type}_128'\n",
    "\n",
    "output_dir = f'/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_2D/{output_type}/'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "# os.makedirs(os.path.join(output_dir, imagesORmasks), exist_ok=True)\n",
    "\n",
    "# Function to process each NIfTI file\n",
    "def process_nifti_file(file_path, output_dir, imagesORmasks):\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.join(output_dir, imagesORmasks), exist_ok=True)\n",
    "\n",
    "    # Load the NIfTI file\n",
    "    img = nib.load(file_path)\n",
    "    img_data = img.get_fdata()\n",
    "\n",
    "    # Apply scaler fit-transform\n",
    "    if imagesORmasks == 'images':\n",
    "        scaler = MinMaxScaler() ## flair ?\n",
    "        img_data = scaler.fit_transform(img_data.reshape(-1, img_data.shape[-1])).reshape(img_data.shape)\n",
    "\n",
    "    # Extract ROI dimensions (assuming ROI_dims is a function that returns the dimensions)\n",
    "    \n",
    "    ## Define this function based on your requirements\n",
    "    # ROI_dims = [[56, 184], [56, 184], [13, 141]]\n",
    "    ROI_range = (slice(56, 184), slice(56, 184), slice(13, 141)) #[56:184, 56:184, 13:141]\n",
    "\n",
    "    # img_data = img_data[roi_dims]\n",
    "    img_data = img_data[ROI_range]\n",
    "\n",
    "    # Convert 3D to 2D slices and save as images\n",
    "    for i in range(img_data.shape[2]):\n",
    "        slice_2d = img_data[:, :, i]\n",
    "        \n",
    "        # Normalize the slice to 0-255 and convert to uint8\n",
    "        # slice_2d = ((slice_2d - slice_2d.min()) / (slice_2d.max() - slice_2d.min()) * 255).astype(np.uint8)\n",
    "        \n",
    "        # Normalize the slice to 0-255 and convert to uint8\n",
    "        if slice_2d.max() != slice_2d.min():\n",
    "            slice_2d = ((slice_2d - slice_2d.min()) / (slice_2d.max() - slice_2d.min()) * 255).astype(np.uint8)\n",
    "        else:\n",
    "            slice_2d = np.zeros_like(slice_2d, dtype=np.uint8)\n",
    "            \n",
    "        img = Image.fromarray(slice_2d)\n",
    "\n",
    "        img.save(os.path.join(output_dir, imagesORmasks, f'{os.path.basename(file_path).replace(\".nii\", \"\")}_slice_{i}.png'))\n",
    "            \n",
    "\n",
    "# # Iterate through all subfolders and process the files\n",
    "# for root, dirs, files in os.walk(input_dir):\n",
    "#     for file in files:\n",
    "#         if file.endswith('t2f.nii'):\n",
    "#             file_path = os.path.join(root, file)\n",
    "#             process_nifti_file(file_path, output_dir, imagesORmasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04ed3956-a427-405f-b1fa-f3dd8c15ad7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test iterator"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "def nifti_file_iterator(input_dir, scan_type):\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            # if file.endswith('t2f.nii'):\n",
    "            if file.endswith(f'{scan_type}.nii'):\n",
    "                yield os.path.join(root, file)\n",
    "\n",
    "# Create an iterator\n",
    "# scan_type = 't2f' #images\n",
    "scan_type = 'seg' #masks\n",
    "file_iterator = nifti_file_iterator(input_dir,scan_type)\n",
    "\n",
    "# # Process each file using the iterator\n",
    "# for file_path in file_iterator:\n",
    "#     print(file_path)\n",
    "#     process_nifti_file(file_path, output_dir, imagesORmasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd0d0e8a-ff26-4063-a161-faf0a855d699",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "images"
    }
   },
   "outputs": [],
   "source": [
    "file_iterator.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3555df8-e1f3-4032-8d58-ab23339e485b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "masks"
    }
   },
   "outputs": [],
   "source": [
    "file_iterator.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "104ecdbf-b1ab-4cc6-9740-9c6a9b0cde5e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test images"
    }
   },
   "outputs": [],
   "source": [
    "# input_fpath = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023-Glioma/BraTS-GLI-00000-000/BraTS-GLI-00000-000-t2f.nii\"\n",
    "\n",
    "# imagesORmasks = 'images'\n",
    "\n",
    "# process_nifti_file(input_fpath, output_dir, imagesORmasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf81dbcb-5679-4974-8bf2-724a6ba0e856",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test masks"
    }
   },
   "outputs": [],
   "source": [
    "input_fpath = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023-Glioma/BraTS-GLI-00000-000/BraTS-GLI-00000-000-seg.nii\"\n",
    "\n",
    "imagesORmasks = 'masks'\n",
    "\n",
    "process_nifti_file(input_fpath, output_dir, imagesORmasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "484bd0d2-a5e3-4458-8a21-c8adcc783367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef534239-c302-4abb-8adb-6e54b514080c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "images"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "# Define the path to the directory containing the sliced images\n",
    "sliced_images_dir = '/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_2D/t2f_128/images'\n",
    "\n",
    "# Function to extract numerical part from the filename for sorting\n",
    "def extract_number(filename):\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    return int(match.group()) if match else 0\n",
    "\n",
    "# Get a list of all image files in the directory\n",
    "# image_files = [f for f in os.listdir(sliced_images_dir) if f.endswith('.png')]\n",
    "\n",
    "# Sort the image files to ensure they are in the correct numerical order\n",
    "# image_files.sort(key=extract_number)\n",
    "image_files = [f'BraTS-GLI-00000-000-t2f_slice_{n}.png' for n in range(128)]\n",
    "\n",
    "# Function to display a grid of images\n",
    "def display_images(image_files, num_images=128):\n",
    "    plt.figure(figsize=(12, 50))\n",
    "    for i, image_file in enumerate(image_files[:num_images]):\n",
    "        img = Image.open(os.path.join(sliced_images_dir, image_file))\n",
    "        plt.subplot(23, 6, i + 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(image_file)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display the first X images\n",
    "display_images(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa765f21-2e3b-4f28-8e02-03bd163caeeb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "masks/seg"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "# Define the path to the directory containing the sliced images\n",
    "sliced_images_dir = '/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_2D/seg_128/masks'\n",
    "\n",
    "# Function to extract numerical part from the filename for sorting\n",
    "def extract_number(filename):\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    return int(match.group()) if match else 0\n",
    "\n",
    "# Get a list of all image files in the directory\n",
    "# image_files = [f for f in os.listdir(sliced_images_dir) if f.endswith('.png')]\n",
    "\n",
    "# Sort the image files to ensure they are in the correct numerical order\n",
    "# image_files.sort(key=extract_number)\n",
    "image_files = [f'BraTS-GLI-00000-000-seg_slice_{n}.png' for n in range(128)]\n",
    "\n",
    "# Function to display a grid of images\n",
    "def display_images(image_files, num_images=128):\n",
    "    plt.figure(figsize=(12, 50))\n",
    "    for i, image_file in enumerate(image_files[:num_images]):\n",
    "        img = Image.open(os.path.join(sliced_images_dir, image_file))\n",
    "        plt.subplot(23, 6, i + 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(image_file)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display the first X images\n",
    "display_images(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8ac88c-eda6-488b-9980-574499f0847e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define the path to the specific image\n",
    "image_path = os.path.join(sliced_images_dir, 'BraTS-GLI-00000-000-seg_slice_60.png')\n",
    "\n",
    "# Open the image using PIL\n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Convert the image to a NumPy array\n",
    "img_array = np.array(img)\n",
    "\n",
    "# Display the shape of the NumPy array\n",
    "print(img_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea059563-f00c-468c-98c9-b56557ce0bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa022239-803e-4816-93b1-ecd04aaf58e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(128):\n",
    "  print(img_array[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93d17fc4-b484-4d80-bca0-91d6f8df51a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "((np.array([0, 85, 170, 255])-0)/255)*3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f28cef6d-ec6f-4cf0-994a-b7a053dd816c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # re.sub(r'\\d+', '{n}', image_files[0])\n",
    "\n",
    "# import os\n",
    "# import re\n",
    "\n",
    "# # Define the path to the directory containing the sliced images\n",
    "# sliced_images_dir = '/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_2D/t2f_128/images'\n",
    "\n",
    "# # Function to replace numeric strings with different letters\n",
    "# def replace_numbers_with_letters(filename):\n",
    "#     return re.sub(r'\\d+', lambda x: chr(97 + int(x.group())), filename)\n",
    "\n",
    "# # Get a list of all image files in the directory\n",
    "# image_files = [f for f in os.listdir(sliced_images_dir) if f.endswith('.png')]\n",
    "\n",
    "# # Extract the main file name pattern and replace numeric strings with letters\n",
    "# pattern_files = [replace_numbers_with_letters(f) for f in image_files]\n",
    "\n",
    "# # Display the modified file names\n",
    "# pattern_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3b2e260-08c4-4554-8143-d9f23234371c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[f'BraTS-GLI-00000-000-t2f_slice_{n}.png' for n in range(128)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4d983bc-0a9f-4a2c-8750-39a0d03f177d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9200a9d0-106f-46f6-a51a-edd1b812bb55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "601ecb62-fc0c-46da-8b06-43cd8488eba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Define the categories\n",
    "categories = [\n",
    "    {\"id\": 1, \"name\": \"NCR\"},   \n",
    "    {\"id\": 2, \"name\": \"ED\"},\n",
    "    {\"id\": 3, \"name\": \"ET\"},\n",
    "    {\"id\": 0, \"name\": \"nonT\"}\n",
    "]\n",
    "\n",
    "# Initialize the COCO dataset structure\n",
    "coco_dataset = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": categories\n",
    "}\n",
    "\n",
    "images_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/val/images/\"\n",
    "# masks_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/val/masks/\"\n",
    "\n",
    "# Assuming images and masks are numpy arrays\n",
    "image_id = 0\n",
    "annotation_id = 0\n",
    "\n",
    "for i in range(len(images)):\n",
    "    image = images[i].numpy()  # Convert Tensor to NumPy array\n",
    "    mask = masks[i].numpy()  # Convert Tensor to NumPy array\n",
    "\n",
    "    # Convert image to uint8 and reshape\n",
    "    image = image.astype(np.uint8).squeeze()\n",
    "\n",
    "    # Ensure the image is 2D or 3D\n",
    "    if image.ndim == 4:\n",
    "        image = image[0, 0]  # Adjust this based on your specific data shape\n",
    "\n",
    "    # Save the image\n",
    "    image_filename = f\"image_{image_id}.npy\"\n",
    "    image_path = os.path.join(images_path, image_filename)\n",
    "    Image.fromarray(image).save(image_path)\n",
    "\n",
    "    # Add image info to COCO dataset\n",
    "    coco_dataset[\"images\"].append({\n",
    "        \"id\": image_id,\n",
    "        \"file_name\": image_filename,\n",
    "        \"height\": image.shape[0],\n",
    "        \"width\": image.shape[1]\n",
    "    })\n",
    "\n",
    "    # Process the mask\n",
    "    for category_id in range(1, 5):\n",
    "        category_mask = (mask == category_id).astype(np.uint8)\n",
    "\n",
    "        # Find contours (assuming OpenCV is available)\n",
    "        contours, _ = cv2.findContours(category_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        for contour in contours:\n",
    "            segmentation = contour.flatten().tolist()\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            area = cv2.contourArea(contour)\n",
    "\n",
    "            # Add annotation info to COCO dataset\n",
    "            coco_dataset[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"segmentation\": [segmentation],\n",
    "                \"area\": area,\n",
    "                \"bbox\": [x, y, w, h],\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    image_id += 1\n",
    "\n",
    "# Save the COCO dataset to a JSON file\n",
    "with open(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed/input_data_128/val/images/instances_train2017.json\", \"w\") as f:\n",
    "    json.dump(coco_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b851585-981d-40f3-9856-ee56f845ca32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "345b0975-ad70-4ec6-8703-c058fc30808b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfefaeae-9fe1-49b2-a874-0c8759f8dfba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48ac0a8a-65e6-4d09-9592-9ebd50fd91e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a712cb57-a353-4dd2-a206-4c1c6242f129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a40769a-045a-400e-b9f0-f1dd4125a692",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get list of images for processing"
    }
   },
   "outputs": [],
   "source": [
    "# t1ce_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*t1c.nii\"))\n",
    "# t2_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*t2w.nii\"))\n",
    "# flair_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*t2f.nii\"))\n",
    "# mask_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*seg.nii\"))\n",
    "\n",
    "\n",
    "# print(\"t1ce list: \", len(t1ce_list))\n",
    "# print(\"t2 list: \", len(t2_list))\n",
    "# print(\"flair list: \", len(flair_list))\n",
    "# print(\"Mask list: \", len(mask_list))\n",
    "\n",
    "# # t1ce list:  625\n",
    "# # t2 list:  625\n",
    "# # flair list:  625\n",
    "# # Mask list:  625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "489de3f2-4e4d-4ca8-b075-8d15d699adda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Save the lists to UC volume\n",
    "# dbutils.fs.put(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/t1ce_list.json\", json.dumps(t1ce_list), overwrite=True)\n",
    "# dbutils.fs.put(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/t2_list.json\", json.dumps(t2_list), overwrite=True)\n",
    "# dbutils.fs.put(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/flair_list.json\", json.dumps(flair_list), overwrite=True)\n",
    "# dbutils.fs.put(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/mask_list.json\", json.dumps(mask_list), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c90e776-6816-4794-9f8d-6c5d72a173f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON files from the UC volume using Unix commands\n",
    "t1ce_list_json = dbutils.fs.head(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/t1ce_list.json\", 1000000)\n",
    "t2_list_json = dbutils.fs.head(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/t2_list.json\", 1000000)\n",
    "flair_list_json = dbutils.fs.head(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/flair_list.json\", 1000000)\n",
    "mask_list_json = dbutils.fs.head(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/mask_list.json\", 1000000)\n",
    "\n",
    "# Parse the JSON strings into Python lists\n",
    "t1ce_list = json.loads(t1ce_list_json)\n",
    "t2_list = json.loads(t2_list_json)\n",
    "flair_list = json.loads(flair_list_json)\n",
    "mask_list = json.loads(mask_list_json)\n",
    "\n",
    "# Print the lengths of the lists\n",
    "print(\"t1ce list: \", len(t1ce_list))\n",
    "print(\"t2 list: \", len(t2_list))\n",
    "print(\"flair list: \", len(flair_list))\n",
    "print(\"Mask list: \", len(mask_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d76e2470-d1ce-4a74-8a3a-60d7d7160066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10077a89-3b0d-4335-97ec-42035e7afe97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to continue with preprocessing for normal pytorch process and then try to convert to coco "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c534c56-7709-4a7f-88f0-0894525fd548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## DATASET Preprocessing test to pytorch dataloader \n",
    "# -- we will need to see how to reformat to coco/yolo friendly format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7c188e-e866-4334-be47-603587e121e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "FOLDER defs"
    }
   },
   "outputs": [],
   "source": [
    "# '/'.join(f\"{DATASET_PATH}\".split(\"/\")[:-2])\n",
    "UCV_folderpath =  \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/\"\n",
    "# UCV_subfoldername = \"BraTS2023_Preprocessed\"\n",
    "# UCV_subfoldername = \"BraTS2023_Preprocessed_v2\" ## without one-hot-mask encoding\n",
    "\n",
    "UCV_subfoldername = \"BraTS2023_Preprocessed_2D\" ## without one-hot-mask encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b12efe-bde2-4be0-b476-8e79bd51ea8a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "to distribute this via spark udf OR streaming?"
    }
   },
   "outputs": [],
   "source": [
    "## Do Once\n",
    "\n",
    "for idx in tqdm(\n",
    "    range(len(t2_list)), desc=\"Preparing to stack, crop and save\", unit=\"file\"\n",
    "):\n",
    "    temp_image_t1ce = nib.load(t1ce_list[idx]).get_fdata()\n",
    "    temp_image_t1ce = scaler.fit_transform(\n",
    "        temp_image_t1ce.reshape(-1, temp_image_t1ce.shape[-1])\n",
    "    ).reshape(temp_image_t1ce.shape)\n",
    " \n",
    "    temp_image_t2 = nib.load(t2_list[idx]).get_fdata()\n",
    "    temp_image_t2 = scaler.fit_transform(\n",
    "        temp_image_t2.reshape(-1, temp_image_t2.shape[-1])\n",
    "    ).reshape(temp_image_t2.shape)\n",
    " \n",
    "    temp_image_flair = nib.load(flair_list[idx]).get_fdata()\n",
    "    temp_image_flair = scaler.fit_transform(\n",
    "        temp_image_flair.reshape(-1, temp_image_flair.shape[-1])\n",
    "    ).reshape(temp_image_flair.shape)\n",
    " \n",
    "    temp_combined_images = np.stack(\n",
    "        [temp_image_flair, temp_image_t1ce, temp_image_t2], axis=3\n",
    "    )\n",
    "    temp_combined_images = temp_combined_images[56:184, 56:184, 13:141]\n",
    "\n",
    "    temp_mask = nib.load(mask_list[idx]).get_fdata()\n",
    "    temp_mask = temp_mask[56:184, 56:184, 13:141]\n",
    " \n",
    "    val, counts = np.unique(temp_mask, return_counts=True)\n",
    " \n",
    "    # If a volume has less than 1% of mask, we simply ignore to reduce computation\n",
    "    if (1 - (counts[0] / counts.sum())) > 0.01:\n",
    "        #         print(\"Saving Processed Images and Masks\")\n",
    "        \n",
    "        ## omit the one_hot encoding ?\n",
    "        if UCV_subfoldername != \"BraTS2023_Preprocessed_v2\":\n",
    "            temp_mask = F.one_hot(torch.tensor(temp_mask, dtype=torch.long), num_classes=4)\n",
    "        \n",
    "        os.makedirs(f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/images\", exist_ok=True)\n",
    "        os.makedirs(f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/masks\", \n",
    "                    exist_ok=True)\n",
    " \n",
    "        np.save(\n",
    "            f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/images/image_\"\n",
    "            + str(idx)\n",
    "            + \".npy\",\n",
    "            temp_combined_images,\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/masks/mask_\"\n",
    "            + str(idx)\n",
    "            + \".npy\",\n",
    "            temp_mask,\n",
    "        )\n",
    " \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7a64bb6-c5da-4059-9c93-c11f07ee3eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "022f2f0e-15bb-40c7-aa4d-54e10348bc77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# compare tqdm vs vectorized pandasUDF for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e3901933-b169-4dee-9309-e407b7462871",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "tbd Vectorize processing"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import nibabel as nib\n",
    "# import os\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from typing import List\n",
    "\n",
    "# # Define the UDF\n",
    "# @pandas_udf(\"string\", PandasUDFType.SCALAR)\n",
    "# def process_images(t1ce_path: pd.Series, \n",
    "#                    t2_path: pd.Series, \n",
    "#                    flair_path: pd.Series, \n",
    "#                    mask_path: pd.Series, \n",
    "#                    idx: pd.Series\n",
    "#                    ) -> pd.Series:\n",
    "#     results: List[str] = []\n",
    "#     for i in range(len(t1ce_path)):\n",
    "#         temp_image_t1ce: np.ndarray = nib.load(t1ce_path[i]).get_fdata()\n",
    "#         temp_image_t1ce = scaler.fit_transform(\n",
    "#             temp_image_t1ce.reshape(-1, temp_image_t1ce.shape[-1])\n",
    "#         ).reshape(temp_image_t1ce.shape)\n",
    "\n",
    "#         temp_image_t2: np.ndarray = nib.load(t2_path[i]).get_fdata()\n",
    "#         temp_image_t2 = scaler.fit_transform(\n",
    "#             temp_image_t2.reshape(-1, temp_image_t2.shape[-1])\n",
    "#         ).reshape(temp_image_t2.shape)\n",
    "\n",
    "#         temp_image_flair: np.ndarray = nib.load(flair_path[i]).get_fdata()\n",
    "#         temp_image_flair = scaler.fit_transform(\n",
    "#             temp_image_flair.reshape(-1, temp_image_flair.shape[-1])\n",
    "#         ).reshape(temp_image_flair.shape)\n",
    "\n",
    "#         temp_mask: np.ndarray = nib.load(mask_path[i]).get_fdata()\n",
    "\n",
    "#         temp_combined_images: np.ndarray = np.stack(\n",
    "#             [temp_image_flair, temp_image_t1ce, temp_image_t2], axis=3\n",
    "#         )\n",
    "\n",
    "#         temp_combined_images = temp_combined_images[56:184, 56:184, 13:141]\n",
    "#         temp_mask = temp_mask[56:184, 56:184, 13:141]\n",
    "\n",
    "#         val, counts = np.unique(temp_mask, return_counts=True)\n",
    "\n",
    "#         # If a volume has less than 1% of mask, we simply ignore to reduce computation\n",
    "#         if (1 - (counts[0] / counts.sum())) > 0.01:\n",
    "#             if UCV_subfoldername != \"BraTS2023_Preprocessed_v2\":\n",
    "#                 temp_mask = F.one_hot(torch.tensor(temp_mask, dtype=torch.long), num_classes=4).numpy()\n",
    "            \n",
    "#             images_dir: str = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/images\"\n",
    "#             masks_dir: str = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/masks\"\n",
    "#             os.makedirs(images_dir, exist_ok=True)\n",
    "#             os.makedirs(masks_dir, exist_ok=True)\n",
    "\n",
    "#             np.save(\n",
    "#                 f\"{images_dir}/image_\" + str(idx[i]) + \".npy\",\n",
    "#                 temp_combined_images,\n",
    "#             )\n",
    "#             np.save(\n",
    "#                 f\"{masks_dir}/mask_\" + str(idx[i]) + \".npy\",\n",
    "#                 temp_mask,\n",
    "#             )\n",
    "#             results.append(\"Processed\")\n",
    "#         else:\n",
    "#             results.append(\"Skipped\")\n",
    "#     return pd.Series(results)\n",
    "\n",
    "# # Create a Spark DataFrame\n",
    "# data = list(zip(t1ce_list, t2_list, flair_list, mask_list, range(len(t1ce_list))))\n",
    "# columns = [\"t1ce_path\", \"t2_path\", \"flair_path\", \"mask_path\", \"idx\"]\n",
    "# df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# # Apply the UDF\n",
    "# df = df.withColumn(\"result\", process_images(\"t1ce_path\", \"t2_path\", \"flair_path\", \"mask_path\", \"idx\"))\n",
    "\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd131a58-4ff3-4fb5-b835-c2e46db19e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images_folder = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/images\"\n",
    "print(len(os.listdir(images_folder)))\n",
    " \n",
    "masks_folder = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/masks\"\n",
    "print(len(os.listdir(masks_folder)))\n",
    "\n",
    "# Images: 575\n",
    "# Masks: 575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73dee558-0381-47ff-92b6-380d2a4250a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "masks_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da6882bc-12c0-4465-91ee-e723e37df953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.listdir(masks_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f53ea15-aef6-4150-8b33-40d5b3f24d00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.listdir(images_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0b458b-6798-4b2c-bb15-3dcc71b26b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.listdir(images_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe104903-9c05-45b7-af47-0aca5fdb31a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.path.join(os.listdir(images_path)[0], 'na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6324dd92-ae3c-4088-9e18-e2da9c5434c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "images_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/val/images/\"\n",
    "jpegs_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/val/jpeg/\"\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(images_path, exist_ok=True)\n",
    "os.makedirs(jpegs_path, exist_ok=True)\n",
    "\n",
    "# Iterate over all .npy files in the images_path directory\n",
    "for npy_file in os.listdir(images_path):\n",
    "    if npy_file.endswith(\".npy\"):\n",
    "        npy_file_path = os.path.join(images_path, npy_file)\n",
    "        \n",
    "        # Load the .npy file\n",
    "        image_array = np.load(npy_file_path)\n",
    "\n",
    "        # Ensure the image array is in the correct format (e.g., uint8)\n",
    "        image_array = image_array.astype(np.uint8)\n",
    "\n",
    "        # Check if the array is 3D\n",
    "        if image_array.ndim == 3:\n",
    "            # Convert the numpy array to a PIL Image\n",
    "            image = Image.fromarray(image_array)\n",
    "            \n",
    "            # Save the image as a JPEG file\n",
    "            jpeg_file_name = npy_file.replace(\".npy\", \".jpeg\")\n",
    "            jpeg_file_path = os.path.join(jpegs_path, jpeg_file_name)\n",
    "            image.save(jpeg_file_path)\n",
    "        else:\n",
    "            print(f\"Skipping file {npy_file} due to incompatible shape: {image_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf45298e-fca1-472b-b3d9-01c6b6a52fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ee35ca-ce8c-4603-a0a1-9a5d6ba77af3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "split into train / val"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "input_folder = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/\"\n",
    " \n",
    "output_folder = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_128/\"\n",
    "\n",
    "## do once [?]\n",
    "splitfolders.ratio(\n",
    "    input_folder, output_folder, seed=42, ratio=(0.75, 0.25), group_prefix=None\n",
    "    # input_folder, output_folder, seed=42, ratio=(0.7, 0.2, 0.1), group_prefix=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aef7ea0e-6966-4018-9ca8-10faac6abd26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3ba008d-830c-4921-a54d-f356a46de86a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "define a custom data loader?"
    }
   },
   "outputs": [],
   "source": [
    "class BraTSDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, normalization=True):\n",
    "        super().__init__()\n",
    " \n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.img_list = sorted(\n",
    "            os.listdir(img_dir)\n",
    "        )  # Ensure sorting to match images and masks\n",
    "        self.mask_list = sorted(os.listdir(mask_dir))\n",
    "        self.normalization = normalization\n",
    " \n",
    "        # If normalization is True, set up a normalization transform\n",
    "        if self.normalization:\n",
    "            self.normalizer = transforms.Normalize(\n",
    "                mean=[0.5], std=[0.5]\n",
    "            )  # Adjust mean and std based on your data\n",
    " \n",
    "    def load_file(self, filepath):\n",
    "        return np.load(filepath)\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "       image_path = os.path.join(self.img_dir, self.img_list[idx])\n",
    "       mask_path = os.path.join(self.mask_dir, self.mask_list[idx])\n",
    "       # Load the image and mask\n",
    "       image = self.load_file(image_path)\n",
    "       mask = self.load_file(mask_path)\n",
    " \n",
    "       # Convert to torch tensors and permute axes to C, D, H, W format (needed for 3D models)\n",
    "       image = torch.from_numpy(image).permute(3, 2, 0, 1)  # Shape: C, D, H, W\n",
    "\n",
    "    #    mask = torch.from_numpy(mask).permute(3, 2, 0, 1)  # Shape: C, D, H, W\n",
    "\n",
    "       mask = torch.from_numpy(mask).permute(2, 0, 1).unsqueeze(0)  # Shape: 1, D, H, W (added channel dimension)   \n",
    "\n",
    "    #    mask = torch.from_numpy(mask).permute(2, 0, 1)  # Shape: D, H, W \n",
    "\n",
    "    #    if masks.dim() == 4:\n",
    "       \n",
    "    #    elif masks.dim() == 3:\n",
    "            # mask = torch.from_numpy(mask).permute(2, 0, 1)  # Shape: D, H, W\n",
    "\n",
    "    #    mask = torch.from_numpy(mask).permute(2, 0, 1).unsqueeze(0)  # Shape: 1, D, H, W (added channel dimension)\n",
    "       \n",
    "       # Normalize the image if normalization is enabled\n",
    "       if self.normalization:\n",
    "           image = self.normalizer(image)\n",
    "       \n",
    "       return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7e63b4b9-f678-4865-8de5-b8f8c980eaf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# class BraTSDataset(Dataset):\n",
    "#     def __init__(self, img_dir, mask_dir, normalization=True):\n",
    "#         super().__init__()\n",
    " \n",
    "#         self.img_dir = img_dir\n",
    "#         self.mask_dir = mask_dir\n",
    "#         self.img_list = sorted(\n",
    "#             os.listdir(img_dir)\n",
    "#         )  # Ensure sorting to match images and masks\n",
    "#         self.mask_list = sorted(os.listdir(mask_dir))\n",
    "#         self.normalization = normalization\n",
    " \n",
    "#         # If normalization is True, set up a normalization transform\n",
    "#         if self.normalization:\n",
    "#             self.normalizer = transforms.Normalize(\n",
    "#                 mean=[0.5], std=[0.5]\n",
    "#             )  # Adjust mean and std based on your data\n",
    " \n",
    "#     def load_file(self, filepath):\n",
    "#         return np.load(filepath)\n",
    " \n",
    "#     def __len__(self):\n",
    "#         return len(self.img_list)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#        image_path = os.path.join(self.img_dir, self.img_list[idx])\n",
    "#        mask_path = os.path.join(self.mask_dir, self.mask_list[idx])\n",
    "#        # Load the image and mask\n",
    "#        image = self.load_file(image_path)\n",
    "#        mask = self.load_file(mask_path)\n",
    " \n",
    "#        # Convert to torch tensors and permute axes to C, D, H, W format (needed for 3D models)\n",
    "#        image = torch.from_numpy(image).permute(3, 2, 0, 1)  # Shape: C, D, H, W\n",
    "#        mask = torch.from_numpy(mask).permute(3, 2, 0, 1)  # Shape: C, D, H, W\n",
    "       \n",
    "#        # Normalize the image if normalization is enabled\n",
    "#        if self.normalization:\n",
    "#            image = self.normalizer(image)\n",
    "       \n",
    "#        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "709e2f17-ba9a-438b-a421-010d21fda061",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LOAD data into dataloader"
    }
   },
   "outputs": [],
   "source": [
    "train_img_dir = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_128/train/images\"\n",
    "train_mask_dir = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_128/train/masks\"\n",
    " \n",
    "val_img_dir = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_128/val/images\"\n",
    "val_mask_dir = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_128/val/masks\"\n",
    " \n",
    "val_img_list = os.listdir(val_img_dir)\n",
    "val_mask_list = os.listdir(val_mask_dir)\n",
    " \n",
    "# Initialize datasets with normalization only\n",
    "train_dataset = BraTSDataset(train_img_dir, train_mask_dir, normalization=True)\n",
    "val_dataset = BraTSDataset(val_img_dir, val_mask_dir, normalization=True)\n",
    " \n",
    "# Print dataset statistics\n",
    "print(\"Total Training Samples: \", len(train_dataset))\n",
    "print(\"Total Val Samples: \", len(val_dataset))\n",
    "\n",
    "#Total Training Samples:  431\n",
    "#Total Val Samples:  144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b678a32-e98e-4db0-8823-6ece6375be25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Data + Check sizes"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False, num_workers=4)\n",
    "\n",
    "# Sanity Check\n",
    "images, masks = next(iter(train_loader))\n",
    "print(f\"Train Image batch shape: {images.shape}\")\n",
    "print(f\"Train Mask batch shape: {masks.shape}\")\n",
    "\n",
    "# Train Image batch shape: torch.Size([5, 3, 128, 128, 128])\n",
    "# Train Mask batch shape: torch.Size([5, 4, 128, 128, 128])\n",
    "\n",
    "\n",
    "# Train Image batch shape: torch.Size([5, 3, 128, 128, 128])\n",
    "# Train Mask batch shape: torch.Size([5, 1, 128, 128, 128])\n",
    "\n",
    "# Train Image batch shape: torch.Size([5, 3, 128, 128, 128])\n",
    "# Train Mask batch shape: torch.Size([5, 128, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c1ee87b-3f80-46a8-9636-c46a8f23f304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "middle_slice = images.shape[2] // 2\n",
    "print(middle_slice)\n",
    "plt.imshow(masks[0, 0, middle_slice, :, :], cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ad3dc6a-b071-4c3a-a5e8-f24642384745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# masks.shape == torch.Size([5, 4, 128, 128, 128])\n",
    "masks.shape == torch.Size([5, 1, 128, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2ce594-b450-44c6-88b7-869e3984a48f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def visualize_slices(images, masks, num_slices=20):\n",
    "    batch_size = images.shape[0]\n",
    " \n",
    "    if masks.shape == torch.Size([5, 4, 128, 128, 128]):\n",
    "        masks = torch.argmax(masks, dim=1)  # along the channel/class dim -- predicted value of each class for that dim (class)\n",
    "        \n",
    "    ## no longer needed when all is already \"combined\" in the mask\n",
    " \n",
    "    for i in range(min(num_slices, batch_size)):\n",
    "        fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    " \n",
    "        middle_slice = images.shape[2] // 2\n",
    "        \n",
    "        ax[0].imshow(images[i, 0, middle_slice, :, :], cmap=\"gray\")\n",
    "        ax[1].imshow(images[i, 1, middle_slice, :, :], cmap=\"gray\")\n",
    "        ax[2].imshow(images[i, 2, middle_slice, :, :], cmap=\"gray\")\n",
    "        \n",
    "        # ax[3].imshow(masks[i, middle_slice, :, :], cmap=\"viridis\")\n",
    "        # ax[4].imshow(masks[i, middle_slice, :, :], cmap=\"gray\")\n",
    "        ax[3].imshow(masks[i,0, middle_slice, :, :], cmap=\"viridis\")\n",
    "        ax[4].imshow(masks[i,0, middle_slice, :, :], cmap=\"gray\")\n",
    "\n",
    "        ax[0].set_title(\"T1ce\")\n",
    "        ax[1].set_title(\"FLAIR\")\n",
    "        ax[2].set_title(\"T2\")\n",
    "        ax[3].set_title(\"Seg Mask\")\n",
    "        ax[4].set_title(\"Mask - Gray\")\n",
    " \n",
    "        plt.show()\n",
    " \n",
    " \n",
    "visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f890ddcb-9d96-489e-81fb-2bdb4a6567f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images, masks = next(iter(train_loader))\n",
    "visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e5583013-de48-4ca4-b41e-2d03212be32b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def visualize_slices(images, masks, num_slices=20):\n",
    "#     batch_size = images.shape[0]\n",
    " \n",
    "#     ## if torch.Size([5, 4, 128, 128, 128])\n",
    "#     masks = torch.argmax(masks, dim=1)  # along the channel/class dim\n",
    " \n",
    "#     for i in range(min(num_slices, batch_size)):\n",
    "#         fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    " \n",
    "#         middle_slice = images.shape[2] // 2\n",
    "#         ax[0].imshow(images[i, 0, middle_slice, :, :], cmap=\"gray\")\n",
    "#         ax[1].imshow(images[i, 1, middle_slice, :, :], cmap=\"gray\")\n",
    "#         ax[2].imshow(images[i, 2, middle_slice, :, :], cmap=\"gray\")\n",
    "                \n",
    "#         ax[3].imshow(masks[i, middle_slice, :, :], cmap=\"viridis\")\n",
    "#         ax[4].imshow(masks[i, middle_slice, :, :], cmap=\"gray\")\n",
    "#         # ax[3].imshow(masks[i][middle_slice, :, :], cmap=\"viridis\")\n",
    "#         # ax[4].imshow(masks[i][middle_slice, :, :], cmap=\"gray\")\n",
    "\n",
    "        \n",
    "#         ax[0].set_title(\"T1ce\")\n",
    "#         ax[1].set_title(\"FLAIR\")\n",
    "#         ax[2].set_title(\"T2\")\n",
    "#         ax[3].set_title(\"Seg Mask\")\n",
    "#         ax[4].set_title(\"Mask - Gray\")\n",
    " \n",
    "#         plt.show()\n",
    " \n",
    " \n",
    "# visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "635c005f-d576-42fd-aa2f-c6845aa7dd7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "masks[3][:,middle_slice,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fbc7490-7b4f-4040-9499-6a51b456fe73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming middle_slice is already defined\n",
    "middle_slice = images.shape[2] // 2\n",
    "\n",
    "# Convert the mask to a 2D array\n",
    "mask_2d = masks[0, 0, middle_slice, :, :]\n",
    "# mask_2d = masks[0][0,middle_slice,:,:]\n",
    "\n",
    "# Plot the 2D mask\n",
    "plt.imshow(mask_2d, cmap=\"viridis\")\n",
    "plt.title(f\"Mask for middle slice {middle_slice}\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9d5280-ca2f-45d4-88ca-f14a39c49c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# images, masks = next(iter(train_loader))\n",
    "# visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2de02897-f985-47f5-a2cc-a6103d6544a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fc36be49-740c-40c9-8fbf-2026d8831207",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Viz"
    }
   },
   "outputs": [],
   "source": [
    "# ## Original Viz code\n",
    "# def visualize_slices(images, masks, num_slices=20):\n",
    "#     batch_size = images.shape[0]\n",
    " \n",
    "#     masks = torch.argmax(masks, dim=1)  # along the channel/class dim\n",
    " \n",
    "#     for i in range(min(num_slices, batch_size)):\n",
    "#         fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    " \n",
    "#         middle_slice = images.shape[2] // 2\n",
    "#         ax[0].imshow(images[i, 0, middle_slice, :, :], cmap=\"gray\")\n",
    "#         ax[1].imshow(images[i, 1, middle_slice, :, :], cmap=\"gray\")\n",
    "#         ax[2].imshow(images[i, 2, middle_slice, :, :], cmap=\"gray\")\n",
    "\n",
    "#         ax[3].imshow(masks[i, middle_slice, :, :], cmap=\"viridis\")\n",
    "#         ax[4].imshow(masks[i, middle_slice, :, :], cmap=\"gray\")\n",
    " \n",
    "#         ax[0].set_title(\"T1ce\")\n",
    "#         ax[1].set_title(\"FLAIR\")\n",
    "#         ax[2].set_title(\"T2\")\n",
    "#         ax[3].set_title(\"Seg Mask\")\n",
    "#         ax[4].set_title(\"Mask - Gray\")\n",
    " \n",
    "#         plt.show()\n",
    " \n",
    " \n",
    "# visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94dee085-aaf3-4d5b-894e-faaa198c75d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bade9fc-8144-49a6-bf2b-182ce1d74fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e7de203-8c09-456e-934d-f616a8c09d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pycocotools\n",
    "!pip install opencv-python-headless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e8879d6-fd5b-4e4a-961b-918ad602f114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb69a513-a6e1-4a45-8b65-d6fed3653ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da523a45-71b3-4d7e-aa4c-c8cbfda3fe46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # 1 for NCR\n",
    "# # 2 for ED\n",
    "# # 3 for ET\n",
    "# # 0 for everything else.\n",
    "\n",
    "# categories = [\"nonT\", \"NCR\", \"ED\", \"ET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6a2116-6ff9-4bfe-bc16-b5cb4052ffca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f37027bc-9ebc-4479-8c4a-67211cbe9fd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Define the categories\n",
    "categories = [\n",
    "    {\"id\": 1, \"name\": \"NCR\"},   \n",
    "    {\"id\": 2, \"name\": \"ED\"},\n",
    "    {\"id\": 3, \"name\": \"ET\"},\n",
    "    {\"id\": 0, \"name\": \"nonT\"}\n",
    "]\n",
    "\n",
    "# Initialize the COCO dataset structure\n",
    "coco_dataset = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": categories\n",
    "}\n",
    "\n",
    "images_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/val/images/\"\n",
    "# masks_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/val/masks/\"\n",
    "\n",
    "# Assuming images and masks are numpy arrays\n",
    "image_id = 0\n",
    "annotation_id = 0\n",
    "\n",
    "for i in range(len(images)):\n",
    "    image = images[i].numpy()  # Convert Tensor to NumPy array\n",
    "    mask = masks[i].numpy()  # Convert Tensor to NumPy array\n",
    "\n",
    "    # Convert image to uint8 and reshape\n",
    "    image = image.astype(np.uint8).squeeze()\n",
    "\n",
    "    # Ensure the image is 2D or 3D\n",
    "    if image.ndim == 4:\n",
    "        image = image[0, 0]  # Adjust this based on your specific data shape\n",
    "\n",
    "    # Save the image\n",
    "    image_filename = f\"image_{image_id}.npy\"\n",
    "    image_path = os.path.join(images_path, image_filename)\n",
    "    Image.fromarray(image).save(image_path)\n",
    "\n",
    "    # Add image info to COCO dataset\n",
    "    coco_dataset[\"images\"].append({\n",
    "        \"id\": image_id,\n",
    "        \"file_name\": image_filename,\n",
    "        \"height\": image.shape[0],\n",
    "        \"width\": image.shape[1]\n",
    "    })\n",
    "\n",
    "    # Process the mask\n",
    "    for category_id in range(1, 5):\n",
    "        category_mask = (mask == category_id).astype(np.uint8)\n",
    "\n",
    "        # Find contours (assuming OpenCV is available)\n",
    "        contours, _ = cv2.findContours(category_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        for contour in contours:\n",
    "            segmentation = contour.flatten().tolist()\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            area = cv2.contourArea(contour)\n",
    "\n",
    "            # Add annotation info to COCO dataset\n",
    "            coco_dataset[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"segmentation\": [segmentation],\n",
    "                \"area\": area,\n",
    "                \"bbox\": [x, y, w, h],\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    image_id += 1\n",
    "\n",
    "# Save the COCO dataset to a JSON file\n",
    "with open(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed/input_data_128/val/images/instances_train2017.json\", \"w\") as f:\n",
    "    json.dump(coco_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88cba90d-6b7b-4109-9a3b-6f500335f319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Define the categories\n",
    "categories = [\n",
    "    {\"id\": 1, \"name\": \"NCR\"},\n",
    "    {\"id\": 2, \"name\": \"ED\"},\n",
    "    {\"id\": 3, \"name\": \"ET\"},\n",
    "    {\"id\": 0, \"name\": \"nonT\"}\n",
    "]\n",
    "\n",
    "# Initialize the COCO dataset structure\n",
    "coco_dataset = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": categories\n",
    "}\n",
    "\n",
    "\n",
    "images_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/val/images/\"\n",
    "# masks_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/val/masks/\"\n",
    "output_json_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/test/instances.json\"\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(images_path, exist_ok=True)\n",
    "os.makedirs(masks_path, exist_ok=True)\n",
    "\n",
    "image_id = 0\n",
    "annotation_id = 0\n",
    "\n",
    "# Iterate over all .npy files in the images_path directory\n",
    "for npy_file in os.listdir(images_path):\n",
    "    if npy_file.endswith(\".npy\"):\n",
    "        npy_file_path = os.path.join(images_path, npy_file)\n",
    "        \n",
    "        # Load the .npy file\n",
    "        image_array = np.load(npy_file_path)\n",
    "\n",
    "        # Ensure the image array is in the correct format (e.g., uint8)\n",
    "        image_array = image_array.astype(np.uint8)\n",
    "\n",
    "        # Check if the array is 3D\n",
    "        if image_array.ndim == 3:\n",
    "            # Convert the numpy array to a PIL Image\n",
    "            image = Image.fromarray(image_array)\n",
    "            \n",
    "            # Save the image as a JPEG file\n",
    "            jpeg_file_name = npy_file.replace(\".npy\", \".jpeg\")\n",
    "            jpeg_file_path = os.path.join(images_path, jpeg_file_name)\n",
    "            image.save(jpeg_file_path)\n",
    "\n",
    "            # Add image info to COCO dataset\n",
    "            coco_dataset[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": jpeg_file_name,\n",
    "                \"height\": image_array.shape[0],\n",
    "                \"width\": image_array.shape[1]\n",
    "            })\n",
    "\n",
    "            # Process the mask\n",
    "            mask_file_path = os.path.join(masks_path, npy_file)\n",
    "            mask_array = np.load(mask_file_path).astype(np.uint8)\n",
    "\n",
    "            for category_id in range(1, 5):\n",
    "                category_mask = (mask_array == category_id).astype(np.uint8)\n",
    "\n",
    "                # Find contours (assuming OpenCV is available)\n",
    "                contours, _ = cv2.findContours(category_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "                for contour in contours:\n",
    "                    segmentation = contour.flatten().tolist()\n",
    "                    x, y, w, h = cv2.boundingRect(contour)\n",
    "                    area = cv2.contourArea(contour)\n",
    "\n",
    "                    # Add annotation info to COCO dataset\n",
    "                    coco_dataset[\"annotations\"].append({\n",
    "                        \"id\": annotation_id,\n",
    "                        \"image_id\": image_id,\n",
    "                        \"category_id\": category_id,\n",
    "                        \"segmentation\": [segmentation],\n",
    "                        \"area\": area,\n",
    "                        \"bbox\": [x, y, w, h],\n",
    "                        \"iscrowd\": 0\n",
    "                    })\n",
    "                    annotation_id += 1\n",
    "\n",
    "            image_id += 1\n",
    "        else:\n",
    "            print(f\"Skipping file {npy_file} due to incompatible shape: {image_array.shape}\")\n",
    "\n",
    "# # Save the COCO dataset to a JSON file\n",
    "# with open(output_json_path, \"w\") as f:\n",
    "#     json.dump(coco_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbd2f64c-5ec1-4eb2-b830-0ae06c087fd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "read_explore_imageNlabels_nii_data_2023_v3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
