{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01d03fa5-5527-4ed3-b0b4-f3f11471db26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## test using non onehot encoded dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093e11ea-7db3-43da-b83e-d7ab85a5a23c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Refs/Notes"
    }
   },
   "outputs": [],
   "source": [
    "# refs \n",
    "# https://learnopencv.com/3d-u-net-brats/#aioseo-dataset-preprocessing (WRT BraTS dataset)\n",
    "# https://github.com/spmallick/learnopencv/tree/master/Training_3D_U-Net_Brain_Tumor_Seg\n",
    "\n",
    "# https://nipy.org/nibabel/nifti_images.html\n",
    "\n",
    "## NIFTI (brain imaging related but not everyone uses it; DICOM may be preferred)\n",
    "# https://github.com/DataCurationNetwork/data-primers/blob/main/Neuroimaging%20DICOM%20and%20NIfTI%20Data%20Curation%20Primer/neuroimaging-dicom-and-nifti-data-curation-primer.md\n",
    "# https://discovery.ucl.ac.uk/id/eprint/10146893/1/geometry_medim.pdf \n",
    "\n",
    "\n",
    "# test data\n",
    "# https://www.kaggle.com/datasets/aiocta/brats2023-part-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eed540dd-12d9-4f36-9da3-a367a4b341ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UC path \n",
    "# mmt_mlops_demos.cv.data\n",
    "# /Volumes/mmt_mlops_demos/cv/data/BraTS2021_00495/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d96c7f-77d7-4b73-8629-13a91d24c711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## to do -- convert some of the setup as a utils/config file  etc. \n",
    "\n",
    "!pip install nibabel -q\n",
    "!pip install scikit-learn -q\n",
    "!pip install tqdm -q\n",
    "!pip install split-folders -q\n",
    "!pip install torchinfo -q\n",
    "!pip install segmentation-models-pytorch-3d -q\n",
    "!pip install livelossplot -q\n",
    "!pip install torchmetrics -q\n",
    "!pip install tensorboard -q\n",
    "\n",
    "!pip install pycocotools\n",
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ad397d-4379-4789-a70f-5121cbedb132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09763bd8-4bc5-4c9e-a5c1-8ef0e88382c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Some of these might not be needed for dataprocessing"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import splitfolders\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import time\n",
    " \n",
    "from dataclasses import dataclass\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda import amp\n",
    " \n",
    "from torchmetrics import MeanMetric\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    " \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchinfo import summary\n",
    "import gc\n",
    " \n",
    "import segmentation_models_pytorch_3d as smp\n",
    " \n",
    "from livelossplot import PlotLosses\n",
    "from livelossplot.outputs import MatplotlibPlot, ExtremaPrinter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0680abd-3782-4d4e-9b29-c8ba0941cc4a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Download test data (BraTS2023) from Kaggle"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/aiocta/brats2023-part-1\n",
    "\n",
    "# Do ONCE\n",
    "# !pip install kaggle -q\n",
    "# !kaggle datasets download -d aiocta/brats2023-part-1 -p /Volumes/mmt_mlops_demos/cv/data/BraTS2023/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86dff492-bcf1-447b-aeb2-198e1da9c1f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Unzip BraTS2023 within UC Vols"
    }
   },
   "outputs": [],
   "source": [
    "# DO ONCE \n",
    "# !sudo apt install unzip\n",
    "# !unzip /Volumes/mmt_mlops_demos/cv/data/BraTS2023/brats2023-part-1.zip -d /Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023-Glioma/\n",
    "\n",
    "# !rm -rf <UD Vols path to>/brats2023-part-1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14cc6b40-b3ca-42d9-8ffe-6d655b50e194",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "maybe useful for later"
    }
   },
   "outputs": [],
   "source": [
    "# def seed_everything(SEED):\n",
    "#    np.random.seed(SEED)\n",
    "#    torch.manual_seed(SEED)\n",
    "#    torch.cuda.manual_seed_all(SEED)\n",
    "#    torch.backends.cudnn.deterministic = True\n",
    "#    torch.backends.cudnn.benchmark = False\n",
    " \n",
    " \n",
    "# def get_default_device():\n",
    "#    gpu_available = torch.cuda.is_available()\n",
    "#    return torch.device('cuda' if gpu_available else 'cpu'), gpu_available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16dbe55f-1ac2-4b7b-9034-72f4b83ff57e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Omitting because we are using YOLO instead"
    }
   },
   "outputs": [],
   "source": [
    "# @dataclass(frozen=True)\n",
    "# class TrainingConfig:\n",
    "#    BATCH_SIZE:      int = 5\n",
    "#    EPOCHS:          int = 100\n",
    "#    LEARNING_RATE: float = 1e-3\n",
    "#    CHECKPOINT_DIR:  str = os.path.join('model_checkpoint', '3D_UNet_Brats2023')\n",
    "#    NUM_WORKERS:     int = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00f1d1e-59bb-46c1-9e30-9fe3fec10ba3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "scaler = MinMaxScaler()"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    " \n",
    "DATASET_PATH = '/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023-Glioma/'\n",
    "print(\"Total Files: \", len(os.listdir(DATASET_PATH)))\n",
    "# Total Files:  625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df60930a-a60c-4a20-a380-5407a1785830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the NIfTI image\n",
    "sample_image_flair = nib.load(os.path.join(DATASET_PATH , \"BraTS-GLI-00000-000/BraTS-GLI-00000-000-t2f.nii\")).get_fdata()\n",
    "print(\"Original max value:\", sample_image_flair.max()) \n",
    "# Original max value: 2934.0\n",
    " \n",
    "# Reshape the 3D image to 2D for scaling\n",
    "sample_image_flair_flat = sample_image_flair.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b36c66c3-21cc-4c12-92f3-ca87b44dae15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_image_flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10cc3f8c-508c-4f3a-9d2b-29b9ae6b8baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_image_flair_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db57ce26-78c7-4632-b5f1-2cf4d0b02774",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "probably want to apply scaling  to the other image types?"
    }
   },
   "outputs": [],
   "source": [
    "# Apply scaling\n",
    "sample_image_flair_scaled = scaler.fit_transform(sample_image_flair_flat)\n",
    " \n",
    "# Reshape it back to the original 3D shape\n",
    "sample_image_flair_scaled = sample_image_flair_scaled.reshape(sample_image_flair.shape)\n",
    " \n",
    "print(\"Scaled max value:\", sample_image_flair_scaled.max())\n",
    "print(\"Shape of scaled Image: \", sample_image_flair_scaled.shape)\n",
    "\n",
    "# Scaled max value: 1.0\n",
    "# Shape of scaled Image:  (240, 240, 155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c776dab-c3b4-44d8-9c8b-8471b672b283",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "MASK info"
    }
   },
   "outputs": [],
   "source": [
    "sample_mask = nib.load(DATASET_PATH + \"BraTS-GLI-00000-000/BraTS-GLI-00000-000-seg.nii\").get_fdata()\n",
    "sample_mask = sample_mask.astype(np.uint8)  \n",
    " \n",
    "print(\"Unique class in the mask\", np.unique(sample_mask)) \n",
    "print(\"Shape of sample_mask: \", sample_mask.shape)\n",
    "\n",
    "# Unique class in the mask [0 1 2 3]\n",
    "# Shape of sample_mask:  (240, 240, 155) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b6d8e8c-af28-4478-91da-6e58bbf57408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_image_t1 = nib.load(DATASET_PATH + \"BraTS-GLI-00000-000/BraTS-GLI-00000-000-t1n.nii\").get_fdata()\n",
    "# sample_image_t1 = sample_image_t1.astype(np.uint8)  # values between 0 and 255 | NOT NEEDED?\n",
    "\n",
    "\n",
    "sample_image_t1ce = nib.load(DATASET_PATH + \"BraTS-GLI-00000-000/BraTS-GLI-00000-000-t1c.nii\").get_fdata()\n",
    "# sample_image_t1c = sample_image_t1c.astype(np.uint8)  # values between 0 and 255 | NOT NEEDED?\n",
    "\n",
    "\n",
    "sample_image_t2 = nib.load(DATASET_PATH + \"BraTS-GLI-00000-000/BraTS-GLI-00000-000-t2w.nii\").get_fdata()\n",
    "# sample_image_t2 = sample_image_t2.astype(np.uint8)  # values between 0 and 255 |  NOT NEEDED?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9febf5b-3ccb-4beb-9700-ac94fd2aed27",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test randint"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the range\n",
    "low = 50\n",
    "high = 90 #141\n",
    "\n",
    "# Generate a random integer between low (inclusive) and high (exclusive)\n",
    "rand_int = np.random.randint(low, high)\n",
    "print(f\"Random integer between {low} and {high}: {rand_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "509d7b1d-16e9-4756-bb43-6ac6c62b7f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# n_slice = random.randint(0, sample_mask.shape[2])  # random slice between 0 - 154\n",
    "n_slice = np.random.randint(low, high) #77\n",
    "print(\"n_slice: \", n_slice)\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    " \n",
    "plt.subplot(231)\n",
    "plt.imshow(sample_image_flair_scaled[:,:,n_slice], cmap='gray')\n",
    "plt.title('Image flair')\n",
    "plt.colorbar()\n",
    " \n",
    "plt.subplot(232)\n",
    "plt.imshow(sample_image_t1[:,:,n_slice], cmap = \"gray\")\n",
    "plt.title(\"Image t1\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.imshow(sample_image_t1ce[:,:,n_slice], cmap='gray')\n",
    "plt.title(\"Image t1ce\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.imshow(sample_image_t2[:,:,n_slice], cmap = 'gray')\n",
    "plt.title(\"Image t2\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.imshow(sample_mask[:,:,n_slice])\n",
    "plt.title(\"Seg Mask\")\n",
    "plt.colorbar()\n",
    " \n",
    "plt.subplot(236)\n",
    "plt.imshow(sample_mask[:,:,n_slice], cmap = 'gray')\n",
    "plt.title('Mask Gray')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c32ae0c-7596-4c6b-96c4-65c817a12d31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_x = np.stack(\n",
    "    [sample_image_flair_scaled, sample_image_t1ce, sample_image_t2], axis=3\n",
    ")  # along the last channel dimension.\n",
    "print(\"Shape of Combined x \", combined_x.shape)\n",
    "\n",
    "# Shape of Combined x  (240, 240, 155, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcf1b5dc-09a6-48ec-8cec-b9f12fa42ed8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## from https://learnopencv.com/3d-u-net-brats/#aioseo-dataset-preprocessing it was determined that the main mask ROIs are within [56:184, 56:184, 13:141] for the dataset -- something to derive from other datasets \n",
    "\n",
    "ROI_dims = [[56, 184], [56, 184], [13, 141]]\n",
    "ROI_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "627eae6e-0163-400e-9cc2-48f8dce2f938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# combined_x = combined_x[56:184, 56:184, 13:141]\n",
    "combined_x = combined_x[ROI_dims[0][0]:ROI_dims[0][1], \n",
    "                        ROI_dims[1][0]:ROI_dims[1][1],\n",
    "                        ROI_dims[2][0]:ROI_dims[2][1]\n",
    "                        ]\n",
    "print(\"Shape after cropping: \", combined_x.shape)\n",
    " \n",
    "# sample_mask_c = sample_mask[56:184,56:184, 13:141]\n",
    "sample_mask_c = sample_mask[ROI_dims[0][0]:ROI_dims[0][1], \n",
    "                            ROI_dims[1][0]:ROI_dims[1][1],\n",
    "                            ROI_dims[2][0]:ROI_dims[2][1]\n",
    "                            ]\n",
    "print(\"Mask shape after cropping: \", sample_mask_c.shape)\n",
    " \n",
    "#Shape after cropping:  (128, 128, 128, 3)\n",
    "#Mask shape after cropping:  (128, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b29f731b-b978-4643-a3da-96eb9c24e461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(list(range(56, 185)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93cbce52-e72a-4c59-9e73-066aea1348bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_mask[:,:,n_slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef13ef91-7f28-4cc5-8661-d158666a9d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_mask[:,:,n_slice].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "154c1141-51ef-4a74-af5b-73cc659ec318",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Curious what this 'looks' like hmm"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(#combined_x[:,:,n_slice-12],\n",
    "           combined_x[:,:,n_slice-ROI_dims[2][0]-1],\n",
    "          #   cmap = 'gray'\n",
    "          )\n",
    "plt.title(\"combined_x\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(#sample_mask_c[:,:,n_slice-12], \n",
    "           sample_mask_c[:,:,n_slice-ROI_dims[2][0]-1]\n",
    "          #  cmap = 'gray'\n",
    "          )\n",
    "plt.title(\"sample_mask_c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5273f73b-5486-4411-8179-d6e73fe80777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://www.synapse.org/Synapse:syn51156910/wiki/622351\n",
    "# Task: Tumor Sub-region Segmentation\n",
    "# The participants are called to address this task by using the provided clinically-acquired training data to develop their method and produce segmentation labels of the different glioma sub-regions. The sub-regions considered for evaluation are the \"enhancing tumor\" (ET), the \"tumor core\" (TC), and the \"whole tumor\" (WT) [see figure below]. The ET is described by areas that show hyper-intensity in T1Gd when compared to T1, but also when compared to “healthy” white matter in T1Gd. The TC describes the bulk of the tumor, which is what is typically resected. The TC entails the ET, as well as the necrotic (NCR) parts of the tumor. The appearance of NCR is typically hypo-intense in T1-Gd when compared to T1. The WT describes the complete extent of the disease, as it entails the TC and the peritumoral edematous/invaded tissue (ED), which is typically depicted by hyper-intense signal in FLAIR.\n",
    "\n",
    "# The provided segmentation labels have values of:\n",
    "\n",
    "# 1 for NCR\n",
    "# 2 for ED\n",
    "# 3 for ET\n",
    "# 0 for everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "590609f1-771d-418a-a1ec-58d28411f66a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for s in range(sample_mask_c.shape[0]):\n",
    "  print(s)\n",
    "  print(sample_mask_c[s,:,n_slice-ROI_dims[2][0]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "149bca19-37af-4ce9-9fc1-f226b498bde9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## one_hot encoding may not be necessary ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd4b971-53eb-4c16-a568-6e842a49dad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_mask_cat  = F.one_hot(torch.tensor(sample_mask_c, dtype = torch.long), num_classes = 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60883c2c-7289-41ce-81d9-16ac91e61fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_mask_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8652f7c4-7068-4eb6-afcc-bfe4f8743e0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for s in range(sample_mask_cat.shape[0]):\n",
    "#   print(s)\n",
    "#   print(sample_mask_cat[:,s,n_slice-ROI_dims[2][0]-1],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a712cb57-a353-4dd2-a206-4c1c6242f129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a40769a-045a-400e-b9f0-f1dd4125a692",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get list of images for processing"
    }
   },
   "outputs": [],
   "source": [
    "# t1ce_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*t1c.nii\"))\n",
    "# t2_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*t2w.nii\"))\n",
    "# flair_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*t2f.nii\"))\n",
    "# mask_list = sorted(glob.glob(f\"{DATASET_PATH}/*/*seg.nii\"))\n",
    "\n",
    "\n",
    "# print(\"t1ce list: \", len(t1ce_list))\n",
    "# print(\"t2 list: \", len(t2_list))\n",
    "# print(\"flair list: \", len(flair_list))\n",
    "# print(\"Mask list: \", len(mask_list))\n",
    "\n",
    "# # t1ce list:  625\n",
    "# # t2 list:  625\n",
    "# # flair list:  625\n",
    "# # Mask list:  625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "489de3f2-4e4d-4ca8-b075-8d15d699adda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Save the lists to UC volume\n",
    "# dbutils.fs.put(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/t1ce_list.json\", json.dumps(t1ce_list), overwrite=True)\n",
    "# dbutils.fs.put(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/t2_list.json\", json.dumps(t2_list), overwrite=True)\n",
    "# dbutils.fs.put(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/flair_list.json\", json.dumps(flair_list), overwrite=True)\n",
    "# dbutils.fs.put(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/mask_list.json\", json.dumps(mask_list), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c90e776-6816-4794-9f8d-6c5d72a173f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON files from the UC volume using Unix commands\n",
    "t1ce_list_json = dbutils.fs.head(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/t1ce_list.json\", 1000000)\n",
    "t2_list_json = dbutils.fs.head(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/t2_list.json\", 1000000)\n",
    "flair_list_json = dbutils.fs.head(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/flair_list.json\", 1000000)\n",
    "mask_list_json = dbutils.fs.head(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/mask_list.json\", 1000000)\n",
    "\n",
    "# Parse the JSON strings into Python lists\n",
    "t1ce_list = json.loads(t1ce_list_json)\n",
    "t2_list = json.loads(t2_list_json)\n",
    "flair_list = json.loads(flair_list_json)\n",
    "mask_list = json.loads(mask_list_json)\n",
    "\n",
    "# Print the lengths of the lists\n",
    "print(\"t1ce list: \", len(t1ce_list))\n",
    "print(\"t2 list: \", len(t2_list))\n",
    "print(\"flair list: \", len(flair_list))\n",
    "print(\"Mask list: \", len(mask_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d76e2470-d1ce-4a74-8a3a-60d7d7160066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10077a89-3b0d-4335-97ec-42035e7afe97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to continue with preprocessing for normal pytorch process and then try to convert to coco "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c534c56-7709-4a7f-88f0-0894525fd548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## DATASET Preprocessing test to pytorch dataloader \n",
    "# -- we will need to see how to reformat to coco/yolo friendly format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7c188e-e866-4334-be47-603587e121e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "FOLDER defs"
    }
   },
   "outputs": [],
   "source": [
    "# '/'.join(f\"{DATASET_PATH}\".split(\"/\")[:-2])\n",
    "UCV_folderpath =  \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/\"\n",
    "# UCV_subfoldername = \"BraTS2023_Preprocessed\"\n",
    "UCV_subfoldername = \"BraTS2023_Preprocessed_v2\" ## without one-hot-mask encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b12efe-bde2-4be0-b476-8e79bd51ea8a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "to distribute this via spark udf OR streaming?"
    }
   },
   "outputs": [],
   "source": [
    "## Do Once\n",
    "\n",
    "for idx in tqdm(\n",
    "    range(len(t2_list)), desc=\"Preparing to stack, crop and save\", unit=\"file\"\n",
    "):\n",
    "    temp_image_t1ce = nib.load(t1ce_list[idx]).get_fdata()\n",
    "    temp_image_t1ce = scaler.fit_transform(\n",
    "        temp_image_t1ce.reshape(-1, temp_image_t1ce.shape[-1])\n",
    "    ).reshape(temp_image_t1ce.shape)\n",
    " \n",
    "    temp_image_t2 = nib.load(t2_list[idx]).get_fdata()\n",
    "    temp_image_t2 = scaler.fit_transform(\n",
    "        temp_image_t2.reshape(-1, temp_image_t2.shape[-1])\n",
    "    ).reshape(temp_image_t2.shape)\n",
    " \n",
    "    temp_image_flair = nib.load(flair_list[idx]).get_fdata()\n",
    "    temp_image_flair = scaler.fit_transform(\n",
    "        temp_image_flair.reshape(-1, temp_image_flair.shape[-1])\n",
    "    ).reshape(temp_image_flair.shape)\n",
    " \n",
    "    temp_mask = nib.load(mask_list[idx]).get_fdata()\n",
    " \n",
    "    temp_combined_images = np.stack(\n",
    "        [temp_image_flair, temp_image_t1ce, temp_image_t2], axis=3\n",
    "    )\n",
    " \n",
    "    temp_combined_images = temp_combined_images[56:184, 56:184, 13:141]\n",
    "    temp_mask = temp_mask[56:184, 56:184, 13:141]\n",
    " \n",
    "    val, counts = np.unique(temp_mask, return_counts=True)\n",
    " \n",
    "    # If a volume has less than 1% of mask, we simply ignore to reduce computation\n",
    "    if (1 - (counts[0] / counts.sum())) > 0.01:\n",
    "        #         print(\"Saving Processed Images and Masks\")\n",
    "        \n",
    "        ## omit the one_hot encoding ?\n",
    "        if UCV_subfoldername != \"BraTS2023_Preprocessed_v2\":\n",
    "            temp_mask = F.one_hot(torch.tensor(temp_mask, dtype=torch.long), num_classes=4)\n",
    "        \n",
    "        os.makedirs(f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/images\", exist_ok=True)\n",
    "        os.makedirs(f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/masks\", \n",
    "                    exist_ok=True)\n",
    " \n",
    "        np.save(\n",
    "            f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/images/image_\"\n",
    "            + str(idx)\n",
    "            + \".npy\",\n",
    "            temp_combined_images,\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/masks/mask_\"\n",
    "            + str(idx)\n",
    "            + \".npy\",\n",
    "            temp_mask,\n",
    "        )\n",
    " \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7a64bb6-c5da-4059-9c93-c11f07ee3eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "022f2f0e-15bb-40c7-aa4d-54e10348bc77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# compare tqdm vs vectorized pandasUDF for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e3901933-b169-4dee-9309-e407b7462871",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "tbd Vectorize processing"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import nibabel as nib\n",
    "# import os\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from typing import List\n",
    "\n",
    "# # Define the UDF\n",
    "# @pandas_udf(\"string\", PandasUDFType.SCALAR)\n",
    "# def process_images(t1ce_path: pd.Series, \n",
    "#                    t2_path: pd.Series, \n",
    "#                    flair_path: pd.Series, \n",
    "#                    mask_path: pd.Series, \n",
    "#                    idx: pd.Series\n",
    "#                    ) -> pd.Series:\n",
    "#     results: List[str] = []\n",
    "#     for i in range(len(t1ce_path)):\n",
    "#         temp_image_t1ce: np.ndarray = nib.load(t1ce_path[i]).get_fdata()\n",
    "#         temp_image_t1ce = scaler.fit_transform(\n",
    "#             temp_image_t1ce.reshape(-1, temp_image_t1ce.shape[-1])\n",
    "#         ).reshape(temp_image_t1ce.shape)\n",
    "\n",
    "#         temp_image_t2: np.ndarray = nib.load(t2_path[i]).get_fdata()\n",
    "#         temp_image_t2 = scaler.fit_transform(\n",
    "#             temp_image_t2.reshape(-1, temp_image_t2.shape[-1])\n",
    "#         ).reshape(temp_image_t2.shape)\n",
    "\n",
    "#         temp_image_flair: np.ndarray = nib.load(flair_path[i]).get_fdata()\n",
    "#         temp_image_flair = scaler.fit_transform(\n",
    "#             temp_image_flair.reshape(-1, temp_image_flair.shape[-1])\n",
    "#         ).reshape(temp_image_flair.shape)\n",
    "\n",
    "#         temp_mask: np.ndarray = nib.load(mask_path[i]).get_fdata()\n",
    "\n",
    "#         temp_combined_images: np.ndarray = np.stack(\n",
    "#             [temp_image_flair, temp_image_t1ce, temp_image_t2], axis=3\n",
    "#         )\n",
    "\n",
    "#         temp_combined_images = temp_combined_images[56:184, 56:184, 13:141]\n",
    "#         temp_mask = temp_mask[56:184, 56:184, 13:141]\n",
    "\n",
    "#         val, counts = np.unique(temp_mask, return_counts=True)\n",
    "\n",
    "#         # If a volume has less than 1% of mask, we simply ignore to reduce computation\n",
    "#         if (1 - (counts[0] / counts.sum())) > 0.01:\n",
    "#             if UCV_subfoldername != \"BraTS2023_Preprocessed_v2\":\n",
    "#                 temp_mask = F.one_hot(torch.tensor(temp_mask, dtype=torch.long), num_classes=4).numpy()\n",
    "            \n",
    "#             images_dir: str = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/images\"\n",
    "#             masks_dir: str = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/masks\"\n",
    "#             os.makedirs(images_dir, exist_ok=True)\n",
    "#             os.makedirs(masks_dir, exist_ok=True)\n",
    "\n",
    "#             np.save(\n",
    "#                 f\"{images_dir}/image_\" + str(idx[i]) + \".npy\",\n",
    "#                 temp_combined_images,\n",
    "#             )\n",
    "#             np.save(\n",
    "#                 f\"{masks_dir}/mask_\" + str(idx[i]) + \".npy\",\n",
    "#                 temp_mask,\n",
    "#             )\n",
    "#             results.append(\"Processed\")\n",
    "#         else:\n",
    "#             results.append(\"Skipped\")\n",
    "#     return pd.Series(results)\n",
    "\n",
    "# # Create a Spark DataFrame\n",
    "# data = list(zip(t1ce_list, t2_list, flair_list, mask_list, range(len(t1ce_list))))\n",
    "# columns = [\"t1ce_path\", \"t2_path\", \"flair_path\", \"mask_path\", \"idx\"]\n",
    "# df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# # Apply the UDF\n",
    "# df = df.withColumn(\"result\", process_images(\"t1ce_path\", \"t2_path\", \"flair_path\", \"mask_path\", \"idx\"))\n",
    "\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd131a58-4ff3-4fb5-b835-c2e46db19e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images_folder = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/images\"\n",
    "print(len(os.listdir(images_folder)))\n",
    " \n",
    "masks_folder = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/masks\"\n",
    "print(len(os.listdir(masks_folder)))\n",
    "\n",
    "# Images: 575\n",
    "# Masks: 575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73dee558-0381-47ff-92b6-380d2a4250a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "masks_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da6882bc-12c0-4465-91ee-e723e37df953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.listdir(masks_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f53ea15-aef6-4150-8b33-40d5b3f24d00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.listdir(images_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0b458b-6798-4b2c-bb15-3dcc71b26b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.listdir(images_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe104903-9c05-45b7-af47-0aca5fdb31a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.path.join(os.listdir(images_path)[0], 'na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6324dd92-ae3c-4088-9e18-e2da9c5434c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "images_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/val/images/\"\n",
    "jpegs_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/val/jpeg/\"\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(images_path, exist_ok=True)\n",
    "os.makedirs(jpegs_path, exist_ok=True)\n",
    "\n",
    "# Iterate over all .npy files in the images_path directory\n",
    "for npy_file in os.listdir(images_path):\n",
    "    if npy_file.endswith(\".npy\"):\n",
    "        npy_file_path = os.path.join(images_path, npy_file)\n",
    "        \n",
    "        # Load the .npy file\n",
    "        image_array = np.load(npy_file_path)\n",
    "\n",
    "        # Ensure the image array is in the correct format (e.g., uint8)\n",
    "        image_array = image_array.astype(np.uint8)\n",
    "\n",
    "        # Check if the array is 3D\n",
    "        if image_array.ndim == 3:\n",
    "            # Convert the numpy array to a PIL Image\n",
    "            image = Image.fromarray(image_array)\n",
    "            \n",
    "            # Save the image as a JPEG file\n",
    "            jpeg_file_name = npy_file.replace(\".npy\", \".jpeg\")\n",
    "            jpeg_file_path = os.path.join(jpegs_path, jpeg_file_name)\n",
    "            image.save(jpeg_file_path)\n",
    "        else:\n",
    "            print(f\"Skipping file {npy_file} due to incompatible shape: {image_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf45298e-fca1-472b-b3d9-01c6b6a52fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ee35ca-ce8c-4603-a0a1-9a5d6ba77af3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "split into train / val"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "input_folder = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_3channels/\"\n",
    " \n",
    "output_folder = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_128/\"\n",
    "\n",
    "## do once [?]\n",
    "splitfolders.ratio(\n",
    "    input_folder, output_folder, seed=42, ratio=(0.75, 0.25), group_prefix=None\n",
    "    # input_folder, output_folder, seed=42, ratio=(0.7, 0.2, 0.1), group_prefix=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aef7ea0e-6966-4018-9ca8-10faac6abd26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3ba008d-830c-4921-a54d-f356a46de86a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "define a custom data loader?"
    }
   },
   "outputs": [],
   "source": [
    "class BraTSDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, normalization=True):\n",
    "        super().__init__()\n",
    " \n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.img_list = sorted(\n",
    "            os.listdir(img_dir)\n",
    "        )  # Ensure sorting to match images and masks\n",
    "        self.mask_list = sorted(os.listdir(mask_dir))\n",
    "        self.normalization = normalization\n",
    " \n",
    "        # If normalization is True, set up a normalization transform\n",
    "        if self.normalization:\n",
    "            self.normalizer = transforms.Normalize(\n",
    "                mean=[0.5], std=[0.5]\n",
    "            )  # Adjust mean and std based on your data\n",
    " \n",
    "    def load_file(self, filepath):\n",
    "        return np.load(filepath)\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "       image_path = os.path.join(self.img_dir, self.img_list[idx])\n",
    "       mask_path = os.path.join(self.mask_dir, self.mask_list[idx])\n",
    "       # Load the image and mask\n",
    "       image = self.load_file(image_path)\n",
    "       mask = self.load_file(mask_path)\n",
    " \n",
    "       # Convert to torch tensors and permute axes to C, D, H, W format (needed for 3D models)\n",
    "       image = torch.from_numpy(image).permute(3, 2, 0, 1)  # Shape: C, D, H, W\n",
    "\n",
    "    #    mask = torch.from_numpy(mask).permute(3, 2, 0, 1)  # Shape: C, D, H, W\n",
    "\n",
    "       mask = torch.from_numpy(mask).permute(2, 0, 1).unsqueeze(0)  # Shape: 1, D, H, W (added channel dimension)   \n",
    "\n",
    "    #    mask = torch.from_numpy(mask).permute(2, 0, 1)  # Shape: D, H, W \n",
    "\n",
    "    #    if masks.dim() == 4:\n",
    "       \n",
    "    #    elif masks.dim() == 3:\n",
    "            # mask = torch.from_numpy(mask).permute(2, 0, 1)  # Shape: D, H, W\n",
    "\n",
    "    #    mask = torch.from_numpy(mask).permute(2, 0, 1).unsqueeze(0)  # Shape: 1, D, H, W (added channel dimension)\n",
    "       \n",
    "       # Normalize the image if normalization is enabled\n",
    "       if self.normalization:\n",
    "           image = self.normalizer(image)\n",
    "       \n",
    "       return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7e63b4b9-f678-4865-8de5-b8f8c980eaf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# class BraTSDataset(Dataset):\n",
    "#     def __init__(self, img_dir, mask_dir, normalization=True):\n",
    "#         super().__init__()\n",
    " \n",
    "#         self.img_dir = img_dir\n",
    "#         self.mask_dir = mask_dir\n",
    "#         self.img_list = sorted(\n",
    "#             os.listdir(img_dir)\n",
    "#         )  # Ensure sorting to match images and masks\n",
    "#         self.mask_list = sorted(os.listdir(mask_dir))\n",
    "#         self.normalization = normalization\n",
    " \n",
    "#         # If normalization is True, set up a normalization transform\n",
    "#         if self.normalization:\n",
    "#             self.normalizer = transforms.Normalize(\n",
    "#                 mean=[0.5], std=[0.5]\n",
    "#             )  # Adjust mean and std based on your data\n",
    " \n",
    "#     def load_file(self, filepath):\n",
    "#         return np.load(filepath)\n",
    " \n",
    "#     def __len__(self):\n",
    "#         return len(self.img_list)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#        image_path = os.path.join(self.img_dir, self.img_list[idx])\n",
    "#        mask_path = os.path.join(self.mask_dir, self.mask_list[idx])\n",
    "#        # Load the image and mask\n",
    "#        image = self.load_file(image_path)\n",
    "#        mask = self.load_file(mask_path)\n",
    " \n",
    "#        # Convert to torch tensors and permute axes to C, D, H, W format (needed for 3D models)\n",
    "#        image = torch.from_numpy(image).permute(3, 2, 0, 1)  # Shape: C, D, H, W\n",
    "#        mask = torch.from_numpy(mask).permute(3, 2, 0, 1)  # Shape: C, D, H, W\n",
    "       \n",
    "#        # Normalize the image if normalization is enabled\n",
    "#        if self.normalization:\n",
    "#            image = self.normalizer(image)\n",
    "       \n",
    "#        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "709e2f17-ba9a-438b-a421-010d21fda061",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LOAD data into dataloader"
    }
   },
   "outputs": [],
   "source": [
    "train_img_dir = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_128/train/images\"\n",
    "train_mask_dir = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_128/train/masks\"\n",
    " \n",
    "val_img_dir = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_128/val/images\"\n",
    "val_mask_dir = f\"{UCV_folderpath}{UCV_subfoldername}/input_data_128/val/masks\"\n",
    " \n",
    "val_img_list = os.listdir(val_img_dir)\n",
    "val_mask_list = os.listdir(val_mask_dir)\n",
    " \n",
    "# Initialize datasets with normalization only\n",
    "train_dataset = BraTSDataset(train_img_dir, train_mask_dir, normalization=True)\n",
    "val_dataset = BraTSDataset(val_img_dir, val_mask_dir, normalization=True)\n",
    " \n",
    "# Print dataset statistics\n",
    "print(\"Total Training Samples: \", len(train_dataset))\n",
    "print(\"Total Val Samples: \", len(val_dataset))\n",
    "\n",
    "#Total Training Samples:  431\n",
    "#Total Val Samples:  144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b678a32-e98e-4db0-8823-6ece6375be25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Data + Check sizes"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False, num_workers=4)\n",
    "\n",
    "# Sanity Check\n",
    "images, masks = next(iter(train_loader))\n",
    "print(f\"Train Image batch shape: {images.shape}\")\n",
    "print(f\"Train Mask batch shape: {masks.shape}\")\n",
    "\n",
    "# Train Image batch shape: torch.Size([5, 3, 128, 128, 128])\n",
    "# Train Mask batch shape: torch.Size([5, 4, 128, 128, 128])\n",
    "\n",
    "\n",
    "# Train Image batch shape: torch.Size([5, 3, 128, 128, 128])\n",
    "# Train Mask batch shape: torch.Size([5, 1, 128, 128, 128])\n",
    "\n",
    "# Train Image batch shape: torch.Size([5, 3, 128, 128, 128])\n",
    "# Train Mask batch shape: torch.Size([5, 128, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c1ee87b-3f80-46a8-9636-c46a8f23f304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "middle_slice = images.shape[2] // 2\n",
    "print(middle_slice)\n",
    "plt.imshow(masks[0, 0, middle_slice, :, :], cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ad3dc6a-b071-4c3a-a5e8-f24642384745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# masks.shape == torch.Size([5, 4, 128, 128, 128])\n",
    "masks.shape == torch.Size([5, 1, 128, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2ce594-b450-44c6-88b7-869e3984a48f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def visualize_slices(images, masks, num_slices=20):\n",
    "    batch_size = images.shape[0]\n",
    " \n",
    "    if masks.shape == torch.Size([5, 4, 128, 128, 128]):\n",
    "        masks = torch.argmax(masks, dim=1)  # along the channel/class dim -- predicted value of each class for that dim (class)\n",
    "        \n",
    "    ## no longer needed when all is already \"combined\" in the mask\n",
    " \n",
    "    for i in range(min(num_slices, batch_size)):\n",
    "        fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    " \n",
    "        middle_slice = images.shape[2] // 2\n",
    "        \n",
    "        ax[0].imshow(images[i, 0, middle_slice, :, :], cmap=\"gray\")\n",
    "        ax[1].imshow(images[i, 1, middle_slice, :, :], cmap=\"gray\")\n",
    "        ax[2].imshow(images[i, 2, middle_slice, :, :], cmap=\"gray\")\n",
    "        \n",
    "        # ax[3].imshow(masks[i, middle_slice, :, :], cmap=\"viridis\")\n",
    "        # ax[4].imshow(masks[i, middle_slice, :, :], cmap=\"gray\")\n",
    "        ax[3].imshow(masks[i,0, middle_slice, :, :], cmap=\"viridis\")\n",
    "        ax[4].imshow(masks[i,0, middle_slice, :, :], cmap=\"gray\")\n",
    "\n",
    "        ax[0].set_title(\"T1ce\")\n",
    "        ax[1].set_title(\"FLAIR\")\n",
    "        ax[2].set_title(\"T2\")\n",
    "        ax[3].set_title(\"Seg Mask\")\n",
    "        ax[4].set_title(\"Mask - Gray\")\n",
    " \n",
    "        plt.show()\n",
    " \n",
    " \n",
    "visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f890ddcb-9d96-489e-81fb-2bdb4a6567f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images, masks = next(iter(train_loader))\n",
    "visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e5583013-de48-4ca4-b41e-2d03212be32b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def visualize_slices(images, masks, num_slices=20):\n",
    "#     batch_size = images.shape[0]\n",
    " \n",
    "#     ## if torch.Size([5, 4, 128, 128, 128])\n",
    "#     masks = torch.argmax(masks, dim=1)  # along the channel/class dim\n",
    " \n",
    "#     for i in range(min(num_slices, batch_size)):\n",
    "#         fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    " \n",
    "#         middle_slice = images.shape[2] // 2\n",
    "#         ax[0].imshow(images[i, 0, middle_slice, :, :], cmap=\"gray\")\n",
    "#         ax[1].imshow(images[i, 1, middle_slice, :, :], cmap=\"gray\")\n",
    "#         ax[2].imshow(images[i, 2, middle_slice, :, :], cmap=\"gray\")\n",
    "                \n",
    "#         ax[3].imshow(masks[i, middle_slice, :, :], cmap=\"viridis\")\n",
    "#         ax[4].imshow(masks[i, middle_slice, :, :], cmap=\"gray\")\n",
    "#         # ax[3].imshow(masks[i][middle_slice, :, :], cmap=\"viridis\")\n",
    "#         # ax[4].imshow(masks[i][middle_slice, :, :], cmap=\"gray\")\n",
    "\n",
    "        \n",
    "#         ax[0].set_title(\"T1ce\")\n",
    "#         ax[1].set_title(\"FLAIR\")\n",
    "#         ax[2].set_title(\"T2\")\n",
    "#         ax[3].set_title(\"Seg Mask\")\n",
    "#         ax[4].set_title(\"Mask - Gray\")\n",
    " \n",
    "#         plt.show()\n",
    " \n",
    " \n",
    "# visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "635c005f-d576-42fd-aa2f-c6845aa7dd7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "masks[3][:,middle_slice,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fbc7490-7b4f-4040-9499-6a51b456fe73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming middle_slice is already defined\n",
    "middle_slice = images.shape[2] // 2\n",
    "\n",
    "# Convert the mask to a 2D array\n",
    "mask_2d = masks[0, 0, middle_slice, :, :]\n",
    "# mask_2d = masks[0][0,middle_slice,:,:]\n",
    "\n",
    "# Plot the 2D mask\n",
    "plt.imshow(mask_2d, cmap=\"viridis\")\n",
    "plt.title(f\"Mask for middle slice {middle_slice}\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9d5280-ca2f-45d4-88ca-f14a39c49c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# images, masks = next(iter(train_loader))\n",
    "# visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2de02897-f985-47f5-a2cc-a6103d6544a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fc36be49-740c-40c9-8fbf-2026d8831207",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Viz"
    }
   },
   "outputs": [],
   "source": [
    "# ## Original Viz code\n",
    "# def visualize_slices(images, masks, num_slices=20):\n",
    "#     batch_size = images.shape[0]\n",
    " \n",
    "#     masks = torch.argmax(masks, dim=1)  # along the channel/class dim\n",
    " \n",
    "#     for i in range(min(num_slices, batch_size)):\n",
    "#         fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    " \n",
    "#         middle_slice = images.shape[2] // 2\n",
    "#         ax[0].imshow(images[i, 0, middle_slice, :, :], cmap=\"gray\")\n",
    "#         ax[1].imshow(images[i, 1, middle_slice, :, :], cmap=\"gray\")\n",
    "#         ax[2].imshow(images[i, 2, middle_slice, :, :], cmap=\"gray\")\n",
    "\n",
    "#         ax[3].imshow(masks[i, middle_slice, :, :], cmap=\"viridis\")\n",
    "#         ax[4].imshow(masks[i, middle_slice, :, :], cmap=\"gray\")\n",
    " \n",
    "#         ax[0].set_title(\"T1ce\")\n",
    "#         ax[1].set_title(\"FLAIR\")\n",
    "#         ax[2].set_title(\"T2\")\n",
    "#         ax[3].set_title(\"Seg Mask\")\n",
    "#         ax[4].set_title(\"Mask - Gray\")\n",
    " \n",
    "#         plt.show()\n",
    " \n",
    " \n",
    "# visualize_slices(images, masks, num_slices=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94dee085-aaf3-4d5b-894e-faaa198c75d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bade9fc-8144-49a6-bf2b-182ce1d74fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e7de203-8c09-456e-934d-f616a8c09d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pycocotools\n",
    "!pip install opencv-python-headless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e8879d6-fd5b-4e4a-961b-918ad602f114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb69a513-a6e1-4a45-8b65-d6fed3653ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da523a45-71b3-4d7e-aa4c-c8cbfda3fe46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # 1 for NCR\n",
    "# # 2 for ED\n",
    "# # 3 for ET\n",
    "# # 0 for everything else.\n",
    "\n",
    "# categories = [\"nonT\", \"NCR\", \"ED\", \"ET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6a2116-6ff9-4bfe-bc16-b5cb4052ffca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f37027bc-9ebc-4479-8c4a-67211cbe9fd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Define the categories\n",
    "categories = [\n",
    "    {\"id\": 1, \"name\": \"NCR\"},\n",
    "    {\"id\": 2, \"name\": \"ED\"},\n",
    "    {\"id\": 3, \"name\": \"ET\"},\n",
    "    {\"id\": 0, \"name\": \"nonT\"}\n",
    "]\n",
    "\n",
    "# Initialize the COCO dataset structure\n",
    "coco_dataset = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": categories\n",
    "}\n",
    "\n",
    "images_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/val/images/\"\n",
    "# masks_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/val/masks/\"\n",
    "\n",
    "# Assuming images and masks are numpy arrays\n",
    "image_id = 0\n",
    "annotation_id = 0\n",
    "\n",
    "for i in range(len(images)):\n",
    "    image = images[i].numpy()  # Convert Tensor to NumPy array\n",
    "    mask = masks[i].numpy()  # Convert Tensor to NumPy array\n",
    "\n",
    "    # Convert image to uint8 and reshape\n",
    "    image = image.astype(np.uint8).squeeze()\n",
    "\n",
    "    # Ensure the image is 2D or 3D\n",
    "    if image.ndim == 4:\n",
    "        image = image[0, 0]  # Adjust this based on your specific data shape\n",
    "\n",
    "    # Save the image\n",
    "    image_filename = f\"image_{image_id}.npy\"\n",
    "    image_path = os.path.join(images_path, image_filename)\n",
    "    Image.fromarray(image).save(image_path)\n",
    "\n",
    "    # Add image info to COCO dataset\n",
    "    coco_dataset[\"images\"].append({\n",
    "        \"id\": image_id,\n",
    "        \"file_name\": image_filename,\n",
    "        \"height\": image.shape[0],\n",
    "        \"width\": image.shape[1]\n",
    "    })\n",
    "\n",
    "    # Process the mask\n",
    "    for category_id in range(1, 5):\n",
    "        category_mask = (mask == category_id).astype(np.uint8)\n",
    "\n",
    "        # Find contours (assuming OpenCV is available)\n",
    "        contours, _ = cv2.findContours(category_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        for contour in contours:\n",
    "            segmentation = contour.flatten().tolist()\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            area = cv2.contourArea(contour)\n",
    "\n",
    "            # Add annotation info to COCO dataset\n",
    "            coco_dataset[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"segmentation\": [segmentation],\n",
    "                \"area\": area,\n",
    "                \"bbox\": [x, y, w, h],\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    image_id += 1\n",
    "\n",
    "# Save the COCO dataset to a JSON file\n",
    "with open(\"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed/input_data_128/val/images/instances_train2017.json\", \"w\") as f:\n",
    "    json.dump(coco_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88cba90d-6b7b-4109-9a3b-6f500335f319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Define the categories\n",
    "categories = [\n",
    "    {\"id\": 1, \"name\": \"NCR\"},\n",
    "    {\"id\": 2, \"name\": \"ED\"},\n",
    "    {\"id\": 3, \"name\": \"ET\"},\n",
    "    {\"id\": 0, \"name\": \"nonT\"}\n",
    "]\n",
    "\n",
    "# Initialize the COCO dataset structure\n",
    "coco_dataset = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": categories\n",
    "}\n",
    "\n",
    "\n",
    "images_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/val/images/\"\n",
    "# masks_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/val/masks/\"\n",
    "output_json_path = \"/Volumes/mmt_mlops_demos/cv/data/BraTS2023/BraTS2023_Preprocessed_v2/input_data_128/test/instances.json\"\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(images_path, exist_ok=True)\n",
    "os.makedirs(masks_path, exist_ok=True)\n",
    "\n",
    "image_id = 0\n",
    "annotation_id = 0\n",
    "\n",
    "# Iterate over all .npy files in the images_path directory\n",
    "for npy_file in os.listdir(images_path):\n",
    "    if npy_file.endswith(\".npy\"):\n",
    "        npy_file_path = os.path.join(images_path, npy_file)\n",
    "        \n",
    "        # Load the .npy file\n",
    "        image_array = np.load(npy_file_path)\n",
    "\n",
    "        # Ensure the image array is in the correct format (e.g., uint8)\n",
    "        image_array = image_array.astype(np.uint8)\n",
    "\n",
    "        # Check if the array is 3D\n",
    "        if image_array.ndim == 3:\n",
    "            # Convert the numpy array to a PIL Image\n",
    "            image = Image.fromarray(image_array)\n",
    "            \n",
    "            # Save the image as a JPEG file\n",
    "            jpeg_file_name = npy_file.replace(\".npy\", \".jpeg\")\n",
    "            jpeg_file_path = os.path.join(images_path, jpeg_file_name)\n",
    "            image.save(jpeg_file_path)\n",
    "\n",
    "            # Add image info to COCO dataset\n",
    "            coco_dataset[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": jpeg_file_name,\n",
    "                \"height\": image_array.shape[0],\n",
    "                \"width\": image_array.shape[1]\n",
    "            })\n",
    "\n",
    "            # Process the mask\n",
    "            mask_file_path = os.path.join(masks_path, npy_file)\n",
    "            mask_array = np.load(mask_file_path).astype(np.uint8)\n",
    "\n",
    "            for category_id in range(1, 5):\n",
    "                category_mask = (mask_array == category_id).astype(np.uint8)\n",
    "\n",
    "                # Find contours (assuming OpenCV is available)\n",
    "                contours, _ = cv2.findContours(category_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "                for contour in contours:\n",
    "                    segmentation = contour.flatten().tolist()\n",
    "                    x, y, w, h = cv2.boundingRect(contour)\n",
    "                    area = cv2.contourArea(contour)\n",
    "\n",
    "                    # Add annotation info to COCO dataset\n",
    "                    coco_dataset[\"annotations\"].append({\n",
    "                        \"id\": annotation_id,\n",
    "                        \"image_id\": image_id,\n",
    "                        \"category_id\": category_id,\n",
    "                        \"segmentation\": [segmentation],\n",
    "                        \"area\": area,\n",
    "                        \"bbox\": [x, y, w, h],\n",
    "                        \"iscrowd\": 0\n",
    "                    })\n",
    "                    annotation_id += 1\n",
    "\n",
    "            image_id += 1\n",
    "        else:\n",
    "            print(f\"Skipping file {npy_file} due to incompatible shape: {image_array.shape}\")\n",
    "\n",
    "# # Save the COCO dataset to a JSON file\n",
    "# with open(output_json_path, \"w\") as f:\n",
    "#     json.dump(coco_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbd2f64c-5ec1-4eb2-b830-0ae06c087fd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "read_explore_imageNlabels_nii_data_2023_v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
